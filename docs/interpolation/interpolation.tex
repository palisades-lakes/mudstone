\chapter{Interpolation}\label{ch:Interpolation}

Motivation: 
\begin{enumerate}
 \item Minimization of real-valued functions on linear spaces.
 \item Iterated line search methods.
 \item $1$d minimization of real-valued functions on a line,
 identified with \glssymbol{RealNumbers}.
 \item $1$d minimization methods based on visiting the
 argmin of a model function.
 \item model function constructed using values and derivatives
 of the $1$d objective function at recently visited locations.
 \item common model function are low degree polynomials 
 interpolating the supplied values and derivatives.
\end{enumerate}

$1$d minimization methods only use the argmin of the model 
function, so constructing a representation of the interpolating 
polynomial is usually unnecessary expense.
This code is intended to make it easy to either 
compute the argmin alone,
or reify the model function, for visualization and debugging.
(Maybe a lazy polynomial constructor that provides argmin with
minimum effort and defers other work until first call to 
\texttt{value} or \texttt{derivative}.)

The polynomials of degree $k$ for a linear space of dimension
$k+1$\cite{wiki:polynomial}.
The most common interpolating polynomials are quadratic or cubic,
so we will have $3$ or $4$ dimensions (degrees of freedom) to play
with. 
(I will discuss affine and constant interpolation for 
completeness.)

Why not higher order? Are other interpolating functions useful?
What about splines?

An interpolating polynomial matches some number of $x,y$
and $x,d$ pairs, where $x$ is a location in the domain,
$y$ a value in the codomain (range) and $d$ the slope.

The polynomials of degree $k$ for a linear space of dimension
$k+1$. 

A degree $k$ polynomial can match a combination of $k+1$ value 
and/or 
slope constraints, where there are at most $k$ slope constraints,
and where the value constraints are at distinct $x$'s and the
slope constraint are at distinct $x$'s, but a slope and a value 
constraint can be given at the same $x$.

A key issue is the choice of basis.

Some bases are may be more convenient when constructing a 
polynomial by interpolation.

Another consideration is the accuracy and cost of 
argmin \texttt{argmin}, \texttt{value}, and \texttt{derivative} 
when the interpolating polynomial is approximated with 
floating point numbers.

\texttt{BigFraction} implementation

Accuracy of evaluating the interpolating polynomial
versus how far the interpolating function is from the function it
interpolates.

Extrapolation often results when using the \texttt{argmin}.
Relationship between \texttt{argmin} and inverse interpolation.

\section{Monomial basis}\label{sec:monomial-basis}

The most common basis is the monomial:
$p(x) = \mu_0 + \mu_1 x + \mu_2 x^2 + \mu_3 x^3$.
$p$ is quadratic if $\mu_3$ is $0$,
affine if $\mu_3$ and $\mu_2$ are $0$,
and constant when only $\mu_0$ is not $0$.
The basis functions are:
\begin{enumerate}[start=0]
 \item $p_0(x) = 1$
 \item $p_1(x) = x$
 \item $p_2(x) = x^2$
 \item $p_3(x) = x^3$
\end{enumerate}

(Note: these basis function aren't orthogonal or normalized)is any
sense. In fact, choosing a distance or an inner product for
a polynomial space is not a simple question.)

Constructing an interpolating polynomial in the monomial basis
requires solving a system of equations.

Suppose, for example, we have $x_0,y_0,d_0$ and
$x_1,y_1,d_1$, that is, we have specified the value and slope we
want at $x_0 \neq x_1$.
That's $4$ constraints, impying a cubic interpolant.
To determine the monomial basis coefficients,
solve the linear system:

\begin{equation}
\begin{pmatrix}
y_0 \\ d_0 \\ y_1 \\ d_1
\end{pmatrix}
=
\begin{pmatrix}
1 & x_0 & \phantom{2} x_0^2 & \phantom{3} x_0^3 \\
  & 1   & 2 x_0 & 3 x_0^2 \\
1 & x_1 & \phantom{2} x_1^2 & \phantom{3} x_1^3 \\
  & 1   & 2 x_1 & 3 x_1^2 
\end{pmatrix}
\begin{pmatrix}
\mu_0 \\ \mu_1 \\ \mu_2 \\ \mu_3
\end{pmatrix}
\end{equation}

\begin{align}
 y_0 & = \mu_0 + \mu_1 x_0 + \phantom{2} \mu_2 x_0^2 + \phantom{3} \mu_3 x_0^3 \\
 d_0 & = \phantom{\mu_0} \phantom{+} \mu_1 \phantom{x_0} + 2 \mu_2 x_0 + 3 \mu_3 x_0^2 \nonumber \\
 y_1 & = \mu_0 + \mu_1 x_1 + \phantom{2} \mu_2 x_1^2 + \phantom{3} \mu_3 x_1^3 \nonumber \\
 d_1 & = \phantom{\mu_0} \phantom{+} \mu_1 \phantom{x_0} + 2 \mu_2 x_1 + 3 \mu_3 x_1^2 \nonumber
\end{align}

\begin{align}
 y_0 = & {\mu_0 + \mu_1 x_0 + \phantom{2} \mu_2 x_0^2 + \phantom{3} \mu_3 x_0^3} \\
 d_0 = & \pushright{\mu_1 \phantom{x_0} + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = & {\mu_0 + \mu_1 x_1 + \phantom{2} \mu_2 x_1^2 + \phantom{3} \mu_3 x_1^3 }\nonumber \\
 d_1 = & \pushright{\mu_1 \phantom{x_0} + 2 \mu_2 x_1 + 3 \mu_3 x_1^2} \nonumber
\end{align}

\begin{align}
 y_0 = & {\mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3} \\
 d_0 = & \pushright{\mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = & {\mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 }\nonumber \\
 d_1 = & \pushright{\mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2} \nonumber
\end{align}

\begin{align}\label{eq:hermite-eqns}
 y_0 & = \mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3 \\
 d_0 & = \mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2 \nonumber \\
 y_1 & = \mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 \nonumber \\
 d_1 & = \mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2 \nonumber 
\end{align}

\begin{align}
 y_0 = &\mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3 \\
 d_0 = &\specialcell{\hfill \mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = &\mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 \nonumber \\
 d_1 = &\specialcell{{\hfill \mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2}} \nonumber 
\end{align}

\subsection{Constant Monomial}

The only possibility is to 'interpolate' $(x_0,y_0)$ with
$\mu_0 = y_0$ (and $\mu_i = 0$ for other $i$). 

\subsection{Affine Monomial}

There are $2$ possibilities, finding the line thru $2$ points,
$(x_0,y_0)$ and $(x_1,y_1)$, or or matching the value and slope at
$(x_0,y_0,d_0)$.

In the first case:
\input{monomial-yy}

In the second:
\input{monomial-yd}

\subsection{Quadratic Monomial}

\begin{equation}
\mu(x) = \mu_0 + \mu_1 x + \mu_2 x^2
\end{equation}

Critical ($\partial\mu(x) = 0$) point at
\begin{equation}
\hat{x} = \frac{-\mu_1}{2 \mu_2}
\end{equation}
$\hat{x}$ is the global minimum if $\mu_2>0$ 
and the global maximum if $\mu_2>0$.
If $\mu_2 = 0$, the global maximum is at 
$\text{sign}(\mu_1)\infty$
and the minimum is at $-\text{sign}(\mu_1)\infty$.

\subsubsection{yyy}

\input{monomial-yyy}

\subsubsection{yyd}

$3$ domain locations:
\input{monomial-yyd3}

$2$ domain locations:
\input{monomial-yyd2}

\subsubsection{ydd}

$3$ domain locations:
\input{monomial-ydd3}

$2$ domain locations:
\input{monomial-ydd2}

\subsection{Cubic Monomial}

\begin{equation}
\mu(x) = \mu_0 + \mu_1 x + \mu_2 x^2 + \mu_3 x^3
\end{equation}

Critical ($\partial\mu(x) = 0$) points are at
\begin{equation}
\hat{x}_{\pm} = \frac{-\mu_2 \pm \sqrt{ \mu_2^{2} - 3 \mu_1 \mu_3 }}{3 \mu_3}
\end{equation}
when $\mu_2^{2} > 3 \mu_1 \mu_3$.
One point is a local minimum, one a local maximum, 
determined by the signs of of the second derivatives
$\partial^2\mu(\hat{x}_{\pm}) = 2 \mu_2 + 6 \mu_3 \hat{x}_{\pm}$.

If $\mu_2^{2} = 3 \mu_1 \mu_3$, 
then $\hat{x} = \hat{x}_{+} = \hat{x}_{-}$ is just a stationary
point, neither a local minimum or maximum.

If $\mu_2^{2} < 3 \mu_1 \mu_3$, then there are no critical points,
no local minima/maxima, and the global minimum (maximum) is at
$\mp\text{sign}(\mu_3)\infty$.

\newgeometry{onecolumn=true}

\subsubsection{yyyy}

\input{monomial-yyyy-1}

\subsubsection{yyyd}

$4$ domain locations:
\input{monomial-yyyd4}

$3$ domain locations:
\input{monomial-yyyd3}

\subsubsection{yydd}

Showing the most likely cases.

$4$ domain locations:
\input{monomial-yydd4}

$2$ domain locations:
\input{monomial-yydd2}

\subsubsection{yddd}
$4$ domain locations:

\input{monomial-yddd4}

\restoregeometry
  
\section{Lagrange basis}

The standard Lagrange form of a polynomial is
\begin{equation}
f(x) = \sum_{i} y_i \prod_{j \neq i} \frac{x - x_j}{x_i -x_j}
\end{equation}

We can interpret this as a linear combination of basis functions,
$\lambda_i (x) = \prod_{j \neq i} \frac{x - x_j}{x_i -x_j}$.
Each $\lambda_i$ is $1$ at $x_i$ and $0$ at all the other $x_j$.
So we can use $\lambda_i$ to satisfy a value constraint at $x_i$
without interferring with value constraints at the other $x_j$.

That suggests that it might be useful to construct similar basis
functions to interpolate combinations of value and slope 
constraints.


\subsection{Affine Lagrange}

\subsubsection{yy}

It's easy enough to see that:
\begin{align}
\lambda^{\text{yy}}_0(x) & = \frac {(x - x_1)} {(x_0 - x_1)} \\
\lambda^{\text{yy}}_1(x) & = \frac {(x - x_0)} {(x_1 - x_0)} \nonumber
\end{align}
satisfies
\begin{align}
\lambda^{\text{yy}}_0(x_0) & = 1 \\
\lambda^{\text{yy}}_0(x_1) & = 0 \nonumber \\
\lambda^{\text{yy}}_1(x_0) & = 0 \nonumber \\
\lambda^{\text{yy}}_1(x_1) & = 1 \nonumber 
\end{align}


\subsubsection{yd}

We need $\partial\lambda^{\text{yd}}_0 (x_1) = 0$,
but that implies affine $\lambda^{\text{yd}}_0$ 
must be a constant.
So we have to extend our definition of Lagrange basis functions
to include constants.
We also need $\lambda^{\text{yd}}_1 (x_0) = 0$ and
$\partial\lambda^{\text{yd}}_1 (x_1) = 1$, leading to the basis:
\begin{align}
\lambda^{\text{yd}}_0 (x) & = 1 \\
\lambda^{\text{yd}}_1 (x) & = x - x_0 \nonumber
\end{align}


\subsection{Quadratic Lagrange}

\subsubsection{yyy}

For example:

\begin{equation}
\lambda^{\text{yyy}}_0(x) = 
\frac {(x - x_1) (x - x_2)} {(x_0 - x_1) (x_0 - x_2)}
\end{equation}

\subsubsection{yyd}

We need 
$\lambda^{\text{yyd}}_0(x_0) = 1$, 
$\lambda^{\text{yyd}}_0(x_1) = 0$, and
$\partial\lambda^{\text{yyd}}_0(x_2) = 0$. 

The second constraint suggests looking at functions of the form
$f(x) = (x - x_1) (x - c)$, which includes all quadratic 
polynomials that are zero at $x_1$, up to a scaling factor.  
$\partial{f}(x) = (x - x_1) + (x - c) = 2 x - x_1 - c$.
$\partial{f}(x_2) = 0$ implies that
$c = 2 x_2 - x_1$,
so
\begin{equation}
\lambda^{\text{yyd}}_0(x) = 
\frac 
{(x - x_1) \left( (x - x_2) + (x_1 - x_2) \right)} 
{(x_0 - x_1) ((x_0 - x_2) + (x_1 - x_2))}
\end{equation}
Note that we may have $x_0 = x_2$ or $x_1 = x_2$, but not both,
and not $x_0 = x_1$ --- otherwise it would be impossible to 
satisfy the constraints.

By symmetry, 
\begin{equation}
\lambda^{\text{yyd}}_1(x) = 
\frac 
{(x - x_0) \left( (x - x_2) + (x_0 - x_2) \right)} 
{(x_1 - x_0) \left( (x_1 - x_2) + (x_0 - x_2) \right)}
\end{equation}

 For $\lambda^{\text{yyd}}_2$, we need 
$\lambda^{\text{yyd}}_2(x_0) = 0$, 
$\lambda^{\text{yyd}}_2(x_1) = 0$, and
$\partial\lambda^{\text{yyd}}_2(x_2) = 1$. 
The standard form $f(x) = (x-x_0) (x-x_1)$ satisfies the first 
$2$ constraints. 
To satisfy the third, we simply need to normailize with
$\partial{f}(x_2) = (x_2 - x_0) + (x_2 - x_1)$:

\begin{equation}
\lambda^{\text{yyd}}_2(x) = 
\frac 
{(x - x_0) (x - x_1)} 
{(x_2 - x_0) + (x_2 - x_1)}
\end{equation}

\subsubsection{ydd}\label{sec:lagrange-ydd}

For $\lambda^{\text{ydd}}_0$ we need:
\begin{align}
\lambda^{\text{ydd}}_0(x_0) & = 1 \\
\partial\lambda^{\text{ydd}}_0(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{ydd}}_0(x_2) & = 0 \nonumber
\end{align}
Because $\lambda^{\text{ydd}}_0$ is quadratic,
$\partial\lambda^{\text{ydd}}_0$ is affine, 
and the only way its values
at $x_1$ and $x_2$ can both be zero is if it is a constant, so 
\begin{equation}
\lambda^{\text{ydd}}_0(x) = 1
\end{equation}

$\lambda^{\text{ydd}}_1$ must satisfy:
\begin{align}
\lambda^{\text{ydd}}_1(x_0) & = 0 \\
\partial\lambda^{\text{ydd}}_1(x_1) & = 1 \nonumber \\
\partial\lambda^{\text{ydd}}_1(x_2) & = 0 \nonumber
\end{align}
This suggests functions proportional to $(x - x_0) (x - c)$ 
for some $c$. Then 
$ 0 = (x_2 - x_0) + (x_2 - c)$
implies $ c = 2 x_2 - x_0$.
Then we normalize with 
$\partial\lambda^{\text{ydd}}_1(x_1) = (x_1 - x_0) + (x_1 - x_2) + (x_0 - x2) 
= 2 (x_1 - x_2)$, so 
\begin{equation}
\lambda^{\text{ydd}}_1(x) = 
\frac {(x - x_0) \left((x - x_2) + (x_0 - x_2)\right)} 
{2 (x_1 - x_2)}
\end{equation}
and, by symmetry,
\begin{equation}
\lambda^{\text{ydd}}_2(x) = 
\frac {(x - x_0) \left( (x -x_1) + (x_0 - x_1) \right)} 
{2 (x_2 - x_1)}
\end{equation}

\subsection{Cubic Lagrange}
 
\subsubsection{yyyy}

For example:

\begin{equation}
\lambda^{\text{yyy}}_0(x) = 
\frac {(x - x_1) (x - x_2) (x - x_3)} 
{(x_0 - x_1) (x_0 - x_2) (x_0 - x_3)}
\end{equation}

\subsubsection{yyyd}

We need 
\begin{align}
\lambda^{\text{yyyd}}_0(x_0) & = 1 \\ 
\lambda^{\text{yyyd}}_0(x_1) & = 0 \nonumber \\
\lambda^{\text{yyyd}}_0(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yyyd}}_0(x_3) & = 0 \nonumber 
\end{align}

Consider functions proportional to $f(x) = (x-x_1)(x-x_2)(x-c_0)$.
This satisfies the first $3$ constraints.
For the fourth, we need
\begin{equation}
0 = (x_3-x_1)(x_3-x_2) + (x_3-x_2)(x_3-c_0) + (x_3-c_0)(x_3-x_1)
\end{equation}
which implies
\begin{align}
c_0 & = 
\frac
{x_3\left((x_3-x_1)+ (x_3-x_2)\right) + (x_3-x_1)(x_3-x_2)}
{(x_3-x_1)+ (x_3-x_2)} 
\\
 &= x_3 + 
\frac
{(x_3-x_1)(x_3-x_2)}
{(x_3-x_1)+ (x_3-x_2)} 
\nonumber
\end{align}

To satisfy the first constraint,
we normalize with $f(x_0)$.
Therefore:
\begin{align}
\lambda^{\text{yyyd}}_0(x) & = 
\frac {(x - x_1) (x - x_2) (x - c_0)} 
{(x_0 - x_1) (x_0 - x_2) (x_0 - c_0))} 
\\
\lambda^{\text{yyyd}}_1(x) & = 
\frac {(x - x_2) (x - x_0) (x - c_1)} 
{(x_1 - x_2) (x_1 - x_0) (x_1 - c_1))} 
\nonumber \\
\lambda^{\text{yyyd}}_2(x) & = 
\frac {(x - x_0) (x - x_1) (x - c_2)} 
{(x_2 - x_0) (x_2 - x_1) (x_2 - c_2))} 
\nonumber \\
\lambda^{\text{yyyd}}_3(x) & = 
\frac {(x - x_0) (x - x_1) (x - x_2)} 
{\left( (x_3 - x_0) (x_3 - x_1) \right)
+ \left( (x_3 - x_1) (x_3 - x_2) \right)
+ \left( (x_3 - x_2) (x_3 - x_0) \right)} 
\nonumber
\end{align}

$\lambda^{\text{yyyd}}_1$ and $\lambda^{\text{yyyd}}_2$ follow by symmetry.
$\lambda^{\text{yyyd}}_3$ is the standard Lagrange form,
normalized for unit slope rather unit value, and fairly obviously
satisfies its constraintz.
 
\subsubsection{yydd}\label{sec:lagrange-yydd}

We need 
\begin{align}
\lambda^{\text{yydd}}_0(x_0) & = 1 \\ 
\lambda^{\text{yydd}}_0(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_0(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_0(x_3) & = 0 \nonumber 
\end{align}

Consider functions of the form $(x-x_1)(x-(a_0+b_0))(x-(a_0-b_)))$.
The second (value) constraint is automatically satisfied.
The slope constraints are satisfied by
% \begin{align}
% a_0 & = \frac{3 (x_2 + x_3) - 2 x_1}{4} \\ 
% b_0 & = \frac{
% \sqrt{3}
% \sqrt{
% - 4 x_1^{2} 
% + 4 x_1 x_2 
% + 4 x_1 x_3 
% + 3 x_2^{2}
% - 10 x_2 x_3
% + 3 x_3^{2}
% }
% }
% {4} \nonumber
% \end{align}
\begin{align}
a_0 & = \frac{3 (x_2 + x_3) - 2 x_1}{4} \\ 
b_0 & = \frac{
\sqrt{ 9 (x_2 - x_3)^2 - 12 (x_1 - x_2) (x_1 -x_3)}
}
{4} \nonumber
\end{align}
(See \texttt{lagrange-yydd.reduce} for an example of using a
symbolic algebra system to solve this.)

By symmetry,  
\begin{align}
\lambda^{\text{yydd}}_0(x) & =
\frac{(x-x_1)(x-(a_0+b_0))(x-(a_0-b_0))}
{(x_0-x_1)(x_0-(a_0+b_0))(x_0-(a_0-b_0))}
\\
\lambda^{\text{yydd}}_1(x) & =
\frac{(x-x_0)(x-(a_1+b_1))(x-(a_1-b_1))}
{(x_1-x_0)(x_1-(a_0+b_1))(x_1-(a_1-b_1))}
\nonumber
\end{align}
where $a_1$ and $b_1$ can be obtained 
by replacing $x_1$ by $x_0$ in the formulas
above.

For $\lambda^{\text{yydd}}_2$ and $\lambda^{\text{yydd}}_3$, 
consider functions of the form $(x-x_0)(x-x_1)(x-c_i)$.
We need, for example, 
\begin{align}
\lambda^{\text{yydd}}_3(x_0) & = 1 \\ 
\lambda^{\text{yydd}}_3(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_3(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_3(x_3) & = 1 \nonumber 
\end{align}
It's reasonably easy to see that 
\begin{align}
c_2 & = x_3 + 
\frac{(x_3 - x_0)(x_3 - x_1)}{(x_3 - x_0) + (x_3 - x_1)} \\
c_3 & = x_2 + 
\frac{(x_2 - x_0)(x_2 - x_1)}{(x_2 - x_0) + (x_2 - x_1)} \\
\end{align}
so
\begin{align}
\lambda^{\text{yydd}}_2(x) & =
\frac{(x-x_0)(x-x_1)(x-c_2)}
{
(x_2-x_0)(x_2-x_1) +
(x_2-x_1)(x_2-c_2) +
(x_2-c_2)(x_2-x_0)}
\\
\lambda^{\text{yydd}}_3(x) & =
\frac{(x-x_0)(x-x_1)(x-x_2)}
{
(x_3-x_0)(x_3-x_1) +
(x_3-x_1)(x_3-c_3) +
(x+3-c_3)(x_3-x_0)}
\nonumber
\end{align}

\paragraph{Hermite interpolation}\label{sec:Hermite-yydd}

When $x_0 = x_2$ and $x_1 = x_3$ --- $2$ knots with value and 
slope specified at each --- 
we have the standard cubic Hermite interpolation
problem\cite{wiki:cubic-hermite-spline}.

The constraints are then:
\begin{align}
\lambda^{\text{hermite}}_{00}(x_0) & = 1 \\ 
\lambda^{\text{hermite}}_{00}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{00}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{00}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{01}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{01}(x_1) & = 1 \nonumber \\
\partial\lambda^{\text{hermite}}_{01}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{01}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{10}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{10}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{10}(x_0) & = 1 \nonumber \\
\partial\lambda^{\text{hermite}}_{10}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{11}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{11}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{11}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{11}(x_1) & = 1 \nonumber 
\end{align}
The $4$ basis functions can be determined by substituting $x_0$
for $x_2$ and $x_1$ for $x_3$ in the results in 
section~\ref{sec:lagrange-yydd}:
\begin{align}
\lambda^{\text{hermite}}_{00}(x) & =
\frac
{(x-x_1)^2 \left(2(x - x_0) + (x_1 - x_0) \right)}
{(x_1-x_0)^3}
\\
\lambda^{\text{hermite}}_{01}(x) & =
\frac
{(x-x_0)^2 \left(2(x - x_1) + (x_0 - x_1) \right)}
{(x_0-x_1)^3}
\nonumber \\
\lambda^{\text{hermite}}_{10}(x) & =
\frac {(x-x_1)^2(x-x_0)} {(x_0-x_1)^2}
\nonumber \\
\lambda^{\text{hermite}}_{11}(x) & =
\frac {(x-x_0)^2(x-x_1)} {(x_1-x_0)^2}
\nonumber
\end{align}

\subsubsection{yddd}\label{sec:lagrange-yddd}

For $\lambda^{\text{yddd}}_0$, we need 
\begin{align}
\lambda^{\text{yddd}}_0(x_0) & = 1 \\ 
\partial\lambda^{\text{yddd}}_0(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{yddd}}_0(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yddd}}_0(x_3) & = 0 \nonumber 
\end{align}
As in section~\ref{sec:lagrange-ydd},
this can only be satisfied by a constant function:
$\lambda^{\text{yddd}}_0(x) = 1$.

The other $3$ basis functions must satisfy variations of
\begin{align}
\lambda^{\text{yddd}}_1(x_0) & = 0 \\ 
\partial\lambda^{\text{yddd}}_1(x_1) & = 1 \nonumber \\
\partial\lambda^{\text{yddd}}_1(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yddd}}_1(x_3) & = 0 \nonumber 
\end{align}
This is essentially the same problem as that of $\lambda^{\text{yydd}}_0$
in section~\ref{sec:lagrange-yydd},
so the solution is:
\begin{align}
\lambda^{\text{yddd}}_1(x) & =
\frac{(x-x_0)(x-(a_1+b_1))(x-(a_1-b_1))}
{(x_1-x_0)(x_1-(a_1+b_1))(x_1-(a_1-b_1))}
\\
\lambda^{\text{yddd}}_2(x) & =
\frac{(x-x_0)(x-(a_2+b_2))(x-(a_2-b_2))}
{(x_2-x_0)(x_2-(a_2+b_2))(x_2-(a_2-b_2))}
\nonumber \\
\lambda^{\text{yddd}}_3(x) & =
\frac{(x-x_0)(x-(a_3+b_3))(x-(a_3-b_3))}
{(x_3-x_0)(x_3-(a_3+b_3))(x_3-(a_3-b_3))}
\nonumber
\end{align}
where
\begin{align}
a_1 & = 
\frac{3 (x_2 + x_3) - 2 x_0}{4} 
\\ 
b_1 & = 
\frac{\sqrt{ 9 (x_2 - x_3)^2 - 12 (x_0 - x_2) (x_0 -x_3)}}{4} 
\nonumber
\end{align}
and similarly for $a_2,b_2$ and $a_3,b_3$.
 



