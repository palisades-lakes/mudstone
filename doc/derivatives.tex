%-----------------------------------------------------------------
\levelstay{Derivatives}
\label{sec:Derivatives}

One way to view the derivative of a function
$\f:\Vspace \mapsto \Wspace$,
at a point $\v$,
is as the linear transformation $\Lmap:\Vspace \mapsto \Wspace$,
that best approximates the local 'slope' of $\f$ at $\v$.
(In the following, $\v$, $\u$, and $\t$ are elements of $\Vspace$.)
To be a little more precise, we want
\begin{equation}
\lim_{ \|{\bf \delta}  \| \mapsto 0}
\frac{ \| \f(\v + {\bf \delta}) - (\f(\v) + \Lmap({\bf\delta})) \|}
{\|{\bf \delta}  \| }
 = 0
\end{equation}
For a concise and correct discussion, see Spivak \cite{spivak-1965}.

Note that for a linear map $\Lmap$,
the derivative is constant over the domain
and the value is $\Lmap$ itself.

\begin{itemize}

\item $\Da{\f}$

In its most general form,
I denote the derivative of $\f$ by $\Da{\f}$.
Note that this is linear-map-valued function of the domain of $\f$.

\item $\Db{\f}{\u}$

I denote the derivative of $\f$ at $\u$ by $\Db{\f}{\u}$.
$\Db{\f}{\u}$ is a specific linear transformation from
the domain of $\f$ to the codomain of $\f$.

\item $\Dc{\f}{\u}{\t}$

The derivative is most often represented by the \textit{Jacobian},
the $m \times n$ matrix of partial derivatives
with respect to some bases for $\Vspace$ and $\Wspace$.
However, it's often easier to express the derivative clearly if we
explicitly include the argument of the linear transformation.
In this case, I write $\Dc{\f}{\u}{\t}$
for the derivative of $f$ at the point $\u$
applied to the vector $\t$.

\item $\Dd{\v_i}{\f}{(\u_0 \ldots \u_{n-1})}{\t_i}$

For functions on direct sum spaces,
$\f(\v_0,\v_1 \ldots \v_{n-1})$, $\v_i \in \Vspace_i$,
it's often easier to consider the derivative
with respect to one argument at a time.
I write $\Dd{\v_i}{\f}{(\u_0 \ldots \u_{n-1})}{\t_0 \ldots \t_{n-1}}$
for the derivative of $\f$ with respect to $\v_i$,
at the point $(\u_0 \ldots \u_{n-1}) \in \oplus_{i=0}^{n-1} \Vspace_i$,
applied to the vector $\t_i \in \Vspace_i$.

\item $\da{j}{\f} = \da{v_j}{\f}$

The traditional partial derivative of $\f$ is with respect to
a single coordinate $v_j$ of the domain.
More formally, this is the directional derivative of $\f$
in the direction of the $j$-th canonical basis vector $\e_j^{\Vspace}$.

$\da{j}{\f}$ is a map from the domain of $\f$ to the co-domain of $\f$.
$\db{j}{\f}{\u}$ is the value of that map at $\u$.
The partial derivative is related to the derivative by
\begin{eqnarray}
\label{eq:partial-full-dervatives}
\Db{\f}{\u}
& = &
\sum_{j=0}^{m-1} \db{j}{\f}{\u} \otimes \e_j^{\Vspace}
\\
\db{j}{\f}{\u}
& = &
\Db{\f}{\u} \e_j^{\Vspace}
\nonumber
\end{eqnarray}

\item $\da{j}{\f_i}$

The Jacobian partial derivatives are the derivatives of
a particular coordinate of $\f$, $\f_i$, with respect to
a single coordinate $v_j$ of the domain.
$\da{j}{\f_i}$ is a real-valued function on the domain of $\f$.
$\db{j}{\f_i}{\u}$ is the value of that function at $\u$.
The Jacobian partial derivatives form the 'matrix' representation of the derivative:
\begin{equation}
\Db{\f}{\u} =
\sum_{i=0}^{m-1}
\sum_{j=0}^{n-1}
\db{j}{\f_i}{\u} \left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\end{equation}

\end{itemize}

In minimizing a real-valued function, $f(\v)$, $\v \in \Vspace$,
we frequently need to know both the direction of maximum increase of $f$
the rate of increase, or slope, of $f$ in that direction.

$\Ga{f}$ is the \textit{gradient} of $f$.
The gradient has a close relationship to the derivative, $\Da{f}$,
and the two are often confused.
Recall that the derivative is a linear transformation
from the domain of $f$ to its codomain.
In the case of real-valued functions,
this means the derivative is a linear function on $\Vspace$,
an element of the dual space of $\Vspace$, a 'row' vector.
It's easy to see that the gradient is simply the dual (the 'transpose')
of the derivative, $\Ga{f} = (\Da{f})^{\dagger}$
(see Spivak \cite{spivak-1965}, p. 96, ex. 4-18).

$\Ga{f}$ maps $\Vspace \mapsto \Vspace$.
$\Gb{f}{\u} \in \Vspace$ is the gradient of $f$ at $\u \in \Vspace$;
it points in the direction of most rapid increase of
$f$ and its magnitude $\| \Gb{f}{\u} \|$ is the
slope of $f$ in that direction.

Notation for the various versions of the gradient
follows that for derivatives:
$\Gc{\v_i}{f}{\u}$ is the partial gradient of $f$ with respect to $\v_i$ at
$\u = \left( \u_0 \ldots \u_{n-1} \right) \in \Vspace = \oplus_{i=0}^{n-1} \Vspace_i$
$\Gc{\v_i}{f}{\u}$ is an element of $\Vspace_i$.

$(\Gb{f}{\u}) \bullet  \t$
and
$(\Gc{\v_i}{f}{\u}) \bullet \t_i$
are the analogs to exressing the derivative as a linear transformation
with an explicit argument.
$(\Gb{f}{\u}) \bullet  \t$ is a real number.
If we take $t$ to be the canonical basis for $\Vspace$
we get an expression for $\Ga{f}$ in terms of the partial derivatives of $f$:
\begin{equation}
\label{eq:gradient-from-partials}
\Gb{f}{\u} = \sum_{j=0}^{m-1} \left( \db{j}{f}{\u} \right) \e_j^{\Vspace}
\end{equation}

$\Ga{\f_i}$ is the gradient of a particular (real-valued) coordinate
of a vector-valued map. It is related to the derivative $\Da{\f}$
in a way simlilar to the relationship between $\Da{\f}$ and its partials $\da{j}{\f}$.
\begin{equation}
\Db{\f}{\u} = \sum_{i=0}^{n-1}  \e_i^{\Wspace} \otimes \Gb{\f_i}{\u}
\end{equation}

The most general identity used in computing derivatives is the \textit{chain rule.}
Suppose
$\f:\Uspace \mapsto \Vspace$,
$\g:\Vspace \mapsto \Wspace$,
and
$\h = \g \circ \f : \Uspace_0 \mapsto \Wspace$
Then
\begin{equation}
\label{eq:chain_rule}
\Db{\h}{\u}
=  \Db{(\g \circ \f)}{\v}
=  \Db{\g}{\f(\v)}  \circ  \Db{\f} {\v}.
\end{equation}

It is sometimes useful to express this in terms of the partial derivatives:
\begin{equation}
\label{eq:chain_rule_partials}
\Db{\h}{\u} =  \sum_{i=0}^{n-1} \db{i}{\g}{\f(\u)} \otimes  \Gb{\f_i}{\u}.
\end{equation}

See Spivak \cite{spivak-1965}, Theorem 2-2.

%-----------------------------------------------------------------
\leveldown{Vector-valued functions}

%-----------------------------------------------------------------
\leveldown{Multilinear functions}
\label{sec:Multilinear-functions}

A map $\f(\v_0 \ldots \v_k):\Vspace_0 \oplus \ldots \oplus \Vspace_k \mapsto \Wspace$
is \textit{multilinear} if
\begin{equation}
\f(a_{00} \v_{00} + a_{01} \v_{01}, \ldots, a_{k0} \v_{k0} + a_{k1} \v_{k1})
 =  \sum_{i_0 \ldots i_k = 0,1} (a_{0i_0} \ldots a_{ki_k}) \f(\v_{0i_0} \ldots \v_{ki_k}).
\end{equation}

The derivative of $\f$
at the point $(\v_0 \ldots \v_k)$, applied to the vector $(\u_0 \ldots \u_k)$ is

\begin{equation}
\Dc{\f}{(\v_0 \ldots \v_k)}{\u_0 \ldots \u_k}
 =  \sum_{i=0,k} \f(\v_0 \ldots \v_{i-1},\u_i,\v_{i+1} \ldots \v_k).
\end{equation}

See Spivak \cite{spivak-1965}, ex. 2-14.

%-----------------------------------------------------------------
\levelstay{Bilinear maps}
\label{sec:Bilinear-functions}

Bilinear functions are a useful special case of multilinear functions.

A function $\f(\v,\u):\Vspace_0 \oplus \Vspace_1 \mapsto \Wspace$
is \textit{bilinear} if
\begin{eqnarray}
\f(a_0 \v_0 + a_1 \v_1, b_0 \u_0 + b_1 \u_1)
& =  & a_0 b_0 f(\v_0,\u_0)
+  a_0 b_1 f(\v_0,\u_1)
\\
& +  & a_1 b_0 f(\v_q,\u_0)
 +  a_1 b_1 f(\v_q,\u_1).
\nonumber
\end{eqnarray}

The derivative of $\f$
at the point $(\v_0,\u_0)$, applied to the vector $(\v,\u)$ is
\begin{equation}
\label{eq:bilinear-derivative}
\Dc{\f}{(\v_0,\u_0)}{\v,\u} = \f(\v_0,\u) + \f(\v,\u_0).
\end{equation}

See Spivak \cite{spivak-1965}, ex. 2-12.

%-----------------------------------------------------------------
\levelstay{Cross products}
\label{sec:cross-products}

We can view the 3-dimensional cross product
$ \times $
as a bilinear function
$\times(\v,\u) = \v \times \u : \Reals^3 \oplus \Reals^3 \mapsto \Reals^3$.
From equation \ref{eq:bilinear-derivative},
$\Dc{\times}{(\v_0,\u_0)}{\v,\u} = \v_0 \times \u + \v \times \u_0$.

Suppose
$\f:\Vspace \mapsto \Reals^3$, and
$\g:\Vspace \mapsto \Reals^3$.
The derivative of $\f \times \g$ is:
\begin{eqnarray}
\Dc{(\f \times \g)}{\v_0}{\v}
& =
& \Db{\times}{(\f(\v_0),\g(\v_0))} \circ (\Dc{\f}{\v_0}{\v}, \Dc{\g}{\v_0}{\v})
\\
& =
& \f(\v_0) \times \Dc{\g}{\v_0}{\v} + \Dc{\f}{\v_0}{\v} \times \g(\v_0) \nonumber
\end{eqnarray}

%-----------------------------------------------------------------
\levelstay{Scalar products}
\label{sec:Scalar-products}

Suppose
$f:\Vspace \mapsto \Reals$, and
$\g:\Vspace \mapsto \Wspace$.
It follows from the chain rule that the derivative of $\h = f\g$ is:
\begin{equation}
\label{eq:scalar_product_derivative}
\Db{(f\g)}{\v} =  f(\v) \Db{\g}{\v} + \g(\v) \otimes \Gb{f}{\v}
\end{equation}

%-----------------------------------------------------------------
\levelstay{Normalized functions}
\label{sec:Normalized-functions}

Let $\tilde{\f}$ be the normalized version of $\f$:
$\tilde{\f}  =  \frac{\f}{\| \f \|}$.
Then, from equations \ref{eq:scalar_product_derivative}
and \ref{eq:norm_derivative}:
\begin{eqnarray}
\Dc{\tilde{\f}}{\v}{\u}
& = &
\Dc{\left( \frac{\f}{\| \f \|}\right)}{\v}{\u}
\\
& = &
\frac{\Dc{\f}{\v}{\u}}{ \| \f(\v) \|}
 +
\f(\v)  \Dc{ \left( \frac{1}{\| \f \|} \right) }{\v}{\u} \nonumber \\
& = &
\frac{\Dc{\f}{\v}{\u}}
{\| \f(\v) \|}
 -
\f(\v)
\frac{\Dc{\| \f \|}{\v}{\u}}
{\|\f(\v)\|^2} \nonumber \\
& = &
\frac{\Dc{\f}{\v}{\u}}{ \| \f(\v) \| }
 -
\f(\v) \left( \frac{\f(\v)^\dagger}{\| \f(\v) \|^3}  \Dc{\f}{\v}{\u} \right) \nonumber \\
& = &
\frac{
\| \f(\v) \|^2 \Dc{\f}{\v}{\u}
 -
\f(\v)\left( \f(\v) \bullet \Dc{\f}{\v}{\u} \right)
}
{\| \f(\v) \|^3}  \nonumber \\
& = &
\frac{\| \f(\v) \|^2 \Identity_{\Wspace} - \left( \f(\v) \otimes \f(\v) \right)  }
{ \| \f(\v) \|^3 }
\Dc{\f}{\v}{\u} \nonumber \\
& = &
\frac{\Identity_{\Wspace} - \left( \tilde{\f}(\v) \otimes \tilde{\f}(\v) \right)  }
{\| \f(\v) \|}
\Dc{\f}{\v}{\u} \nonumber
\end{eqnarray}


We can write the derivative above without reference to the argument $\u$:
\begin{equation}
\label{eq:normalized_function_derivative}
\Db{\tilde{\f}}{\v}
 =
\Db{\left( \frac{\f}{\| \f \|} \right)}{\v}
 =
\frac{\Identity_{\Wspace} - \left( \tilde{\f}(\v) \otimes \tilde{\f}(\v) \right) }
{ \| \f(\v) \| }
\Db{\f}{\v}
\end{equation}

A common, trivial, normalized function is the normalized version of
a vector: $\tilde{\v} =  \frac{\v}{ \| \v \| }$.

From equation \ref{eq:normalized_function_derivative}
it follows that:
\begin{equation}
\label{eq:normalized_vector_derivative}
\Db{\tilde{\v}}{\u}
 =
\Db{ \left( \frac{\v}{ \| \v \| } \right) }{\u}
 =
\frac{\Identity_{\Vspace} - \left( \tilde{\u} \otimes \tilde{\u} \right) }
{ \| \u \| }
 =
\frac{\| \u \|^2 \Identity_{\Vspace} - \left( \u \otimes \u \right) }
{\| \u \|^3}
\end{equation}

%-----------------------------------------------------------------
\levelup{Real-valued functions}
\label{sec:Real-valued-functions}

%-----------------------------------------------------------------
\leveldown{Inner products}
\label{sec:Inner-products}

We can view the inner product on $\Vspace$, $\v \bullet \u$,
as a bilinear function $\bullet(\v,\u) : \Vspace \oplus \Vspace \mapsto \Reals$.
Thus
\begin{equation}
\Dc{\bullet}{(\v_0,\u_0)}{\v,\u} = \v_0 \bullet \u + \v \bullet \u_0.
\end{equation}

Suppose
$\f:\Vspace \mapsto \Vspace$, and
$\g:\Vspace \mapsto \Vspace$.
The derivative of $\f \bullet \g$ is:
\begin{eqnarray}
\label{eq:dot_derivative}
\Dc{(\f \bullet \g)}{\v_0}{\v}
& =
& \Db{\bullet}{(\f(\v_0),\g(\v_0))} \circ (\Dc{\f}{\v_0}{\v}, \Dc{\g}{\v_0}{\v})
\\
& =
& \f(\v_0) \bullet \Dc{\g}{\v_0}{\v}  +  \g(\v_0) \bullet \Dc{\f}{\v_0}{\v} \nonumber
\end{eqnarray}

See Spivak \cite{spivak-1965}, ex. 2-13.

%-----------------------------------------------------------------
\levelstay{Angles}
\label{sec:Angles}

The angle between 2 vectors $\v_0, \v_1 \in \Vspace$,
is the inverse cosine of their normalized inner product:
$\theta(\v_0,\v_1)
=
\cos^{-1} \left( \frac{ \v_0 \bullet \v_1 } {\|\v_0\| \|\v_1\|} \right)$.
Recall that the derivative of the $\cos^{-1}$ is
$\frac{\mathrm d}{\mathrm dx} \cos^{-1}(x) = \frac{-1}{\sqrt{1 - x^2} }$.
It follows that:
\begin{eqnarray*}
\Gc{\v_0}{\theta(\v_0,\v_1)}{\u}
& = &
\frac{-1}
{ \sqrt{1 - \left( \frac{\u_0 \bullet \u_1}{\| \u_0 \| \| \u_1 \|} \right)^2 }}
\Gc{\v_0}{\left( \frac{\u_0 \bullet \u_1}{\| \u_0 \| \| \u_1 \|} \right)}{\u}
\\
& = &
\frac{-\|\u_0\|\|\u_1\|}
{ \sqrt{\|\u_0\|^2\|\u_1\|^2 - \left( \u_0 \bullet \u_1 \right)^2 }}
\left[
\frac{\u_1}{\|\u_0\|\|\u_1\|}
+
\frac{\left( \u_0 \bullet \u_1 \right)}{\| \u1 \|}
\Gc{\v_0}{\left( \frac{1}{\| \v_0 \|} \right)} {\u}
\right]
\nonumber
\\
& = &
\frac{-\|\u_0\|\|\u_1\|}
{ \sqrt{\|\u_0\|^2\|\u_1\|^2 - \left( \u_0 \bullet \u_1 \right)^2 }}
\left[
\frac{\u_1}{\|\u_0\|\|\u_1\|}
-
\frac{\left( \u_0 \bullet \u_1 \right) \u0}{\| \u1 \| \|\u_0\|^3}
\right]
\nonumber
\\
& = &
\frac{-1}
{ \sqrt{\|\u_0\|^2\|\u_1\|^2 - \left( \u_0 \bullet \u_1 \right)^2 }}
\left[
\u_1
-
\frac{\left( \u_0 \bullet \u_1 \right) \u0}{\|\u_0\|^2}
\right]
\nonumber
\end{eqnarray*}
which results in
\begin{eqnarray}
\label{eq:angle_gradient}
\Gc{\v_0}{\theta(\v_0,\v_1)}{\u}
& = &
\frac{- \u_1 \perp \u_0}
{ \sqrt{\|\u_0\|^2\|\u_1\|^2 - \left( \u_0 \bullet \u_1 \right)^2 }}
\\
\Gc{\v_1}{\theta(\v_0,\v_1)}{\u}
& = &
\frac{- \u_0 \perp \u_1}
{ \sqrt{\|\u_0\|^2\|\u_1\|^2 - \left( \u_0 \bullet \u_1 \right)^2 }}
\nonumber
\end{eqnarray}

%-----------------------------------------------------------------
\levelstay{Euclidean norm}
\label{sec:Euclidean-norm}

Let $l_2(\v) = \| \v  \|: \Vspace \mapsto \Reals$
be the usual euclidean norm on $\Vspace$.
Let $l_2^2(\v) = \| \v  \|^2 $
be its square and $ \| \v  \|^3$ the cube.
\begin{eqnarray}
\label{eq:l2-gradient}
\Gb{l_2}{\v} = \frac{ \v }{ \| \v  \|} &
\Gb{l_2^2}{\v} =  2\v &
\Gb{l_2^3}{\v} = 3 \| \v  \| \v \\
\Db{l_2}{\v} = \frac{ \v^\dagger }{ \| \v  \|} &
\Db{l_2^2}{\v} = 2\v{^\dagger} &
\Db{l_2^3}{\v} = 3 \| \v  \| \v^\dagger \nonumber
\end{eqnarray}

Let $\f(\v) : \Vspace \mapsto \Wspace$.
By the chain rule:
$\Db{\| \f \|^2}{\v}  =  2 {\f(\v)}^{\dagger} \Db{\f}{\v} $
and
$\Gb{\| \f \|^2}{\v}  =  2 \Db{\f}{\v}^\dagger \circ \f(\v)$.
\begin{eqnarray}
\label{eq:norm_derivative}
\Db{\| \f \|}{\v}
& = &
\frac{\f(\v)^\dagger}{\| \f(\v) \|} \Db{\f}{\v}  \\
\Gb{\| \f \|}{\v}
& = &
\left(\Db{\f}{\v}\right)^\dagger \circ  \frac{\f(\v)}{ \| \f(\v)  \|}
\label{eq:norm_gradient}
\end{eqnarray}

%-----------------------------------------------------------------
\levelup{Linear-function-valued functions}
\label{sec:Linear-function-valued-functions}

The set of linear functions between two inner product spaces
$\{ \Lmap : \Vspace \mapsto \Wspace \}$
is itself a inner product space $\Lspace(\Vspace,\Wspace)$,
with the inner product defined by
$\Lmap \bullet \Mmap = \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} \Lmap_{ij} \Mmap_{ij}$.
The set of linear functions
$\Emap_{ij}^{\Lspace(\Vspace,\Wspace)}  = \e_i^{\Wspace} \otimes \e_j^{\Vspace}$
are the canonical basis vectors for $\Lspace(\Vspace,\Wspace)$.

If $\f$ is a function between spaces of linear functions,
$\f : \Lspace(\Vspace_0,\Wspace_0) \mapsto \Lspace(\Vspace_1,\Wspace_1)$,
its derivative, $\Da{\f}$,
is a function from a space of linear functions
to a space of linear functions between two
spaces of linear functions:
$\Da{\f} : \Lspace(\Vspace_0,\Wspace_0) \mapsto
\Lspace(\Lspace(\Vspace_0,\Wspace_0), \Lspace(\Vspace_1,\Wspace_1))$.
This can get a little confusing,
and it often helps to consider both the partial derivatives of $\f$
and the gradients of the coordinates of $\f$,
which can make it easier to apply the chain rule to
compositions of functions of functions via equation \ref{eq:chain_rule_partials}.

$\da{ij}{\f}$ is the partial derivative with respect to its $ij$-th matrix coordinate,
that is, the directional derivative of $\f$ in the direction
of the $ij$-th canonical basis vector, $\Emap_{ij}^{\Lspace(\Vspace_0,\Wspace_0)}$.
As usual the value of the partial derivative at a specific
$\Lmap_0 \in  \Lspace(\Vspace_0,\Wspace_0)$,
$\db{ij}{\f}{\Lmap_0}$ is an element of the co-domain of $\f$,
a linear function in  $\Lspace(\Vspace_1,\Wspace_1)$.

$\Ga{\f_{kl}}$ is the gradient of the $kl$-th matrix coordinate of the value of $\f$.
As usual, the value of the gradient at a specific $\Lmap_0$,
$\Gb{\f_{kl}}{\Lmap_0}$ is an element of the domain of $\f$,
a linear function in $\Lspace(\Vspace_0,\Wspace_0)$.

Note that nether of these are elements of the Jacobian of $\f$,
which needs 4 indexes: $\da{ij}{\f_{kl}}$.

I am particularly interested in computing the derivative of the
pseudo-inverse: $\Pseudoinverse(\Lmap) \equiv \Lmap^{-}$.
The set of full rank linear functions is an open set,
and we can define the derivative of $\Pseudoinverse(\Lmap)$ there.
For full rank linear functions,
we can use the chain rule and the identity
$\Lmap^{-} = \left( \Lmap^{\dagger} \Lmap \right)^{-1} \Lmap^{\dagger}$
(equation \ref{eq:full-rank-pseudo-inverse})
to compute the derivative of the pseudo-inverse
(section \ref{sec:Pseudo-inverse}).

To do this I will first establish partial derivatives and gradients of:
\begin{align}
\label{eq:transpose-derivative}
&\Transpose(\Lmap) \equiv \Lmap^{\dagger}
&&\db{ij}{\Transpose}{\Lmap} =  \e_j^{\Vspace} \otimes \e_i^{\Wspace}
\forall \Lmap
\\
&\h( \Lmap ) = \f ( \Lmap ) \g ( \Lmap )
&&\text{See section \ref{sec:Map-product} }
\\
&\LTL(\Lmap) \equiv \Lmap^{\dagger} \Lmap
&&\text{See section \ref{sec:LTL} }
\\
&\Inverse(\Lmap) \equiv \Lmap^{-1}
&&\text{See section \ref{sec:Inverse} }
\end{align}

%-----------------------------------------------------------------
\leveldown{Map product}
\label{sec:Map-product}

Let
$\f : \Lspace(\Vspace_0,\Wspace_0) \mapsto \Lspace(\Vspace_1,\Wspace_1)$,
$\g : \Lspace(\Vspace_0,\Wspace_0) \mapsto \Lspace(\Uspace_1,\Vspace_1)$,
and
$\h = \f\g : \Lspace(\Vspace_0,\Wspace_0) \mapsto \Lspace(\Uspace_1,\Wspace_1)$.
Note that
$\db{ij}{\f}{\Lmap} \in  \Lspace(\Vspace_1,\Wspace_1)$,
$\db{ij}{\g}{\Lmap} \in  \Lspace(\Uspace_1,\Vspace_1)$,
and
$\db{ij}{\h}{\Lmap} \in  \Lspace(\Uspace_1,\Wspace_1)$.
Consider the matrix representation of $\db{ij}{\h}{\Lmap}$:
\begin{eqnarray}
\left( \db{ij}{\h}{\Lmap} \right)_{kl}
& = &
\db{ij}{\h_{kl}}{\Lmap}
\\
& = &
\db{ij}{\left( \sum_{m} \f_{km} \g_{ml} \right)}{\Lmap}
\nonumber
\\
& = &
\sum_{m}  \left[
\left( \db{ij}{\f_{km}}{\Lmap} \right) \g_{ml}(\Lmap)
+
\f_{km}(\Lmap) \left( \db{ij}{\g_{ml}}{\Lmap} \right)
\right]
\nonumber
\\
& = &
\left[
\left( \db{ij}{\f}{\Lmap} \right) \g(\Lmap)
+
\f(\Lmap) \left( \db{ij}{\g}{\Lmap} \right)
\right]_{kl}
\nonumber
\end{eqnarray}
Therefore
\begin{equation}
\label{eq:function-product-derivative}
\db{ij}{\h}{\Lmap}
 =
\left( \db{ij}{\f}{\Lmap} \right) \g(\Lmap)
+
\f(\Lmap) \left( \db{ij}{\g}{\Lmap} \right)
\end{equation}

%-----------------------------------------------------------------
\leveldown{$\Lmap^{\dagger} \Lmap$}
\label{sec:LTL}

A simple function on linear functions
is $\LTL(\Lmap) \equiv \Lmap^{\dagger} \Lmap
: \Lspace(\Vspace,\Wspace) \mapsto \Lspace(\Vspace,\Vspace)$.

The partial derivative is computed using equations
\ref{eq:transpose-derivative}
and
\ref{eq:function-product-derivative}:

\begin{equation}
\db{ij}{\LTL}{\Lmap}
=
\left( \e_j^{\Vspace} \otimes \e_i^{\Wspace} \right) \Lmap
+
\Lmap^{\dagger} \left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
=
\left( \e_j^{\Vspace} \otimes \r_i^{\Lmap} \right)
+
\left( \r_i^{\Lmap} \otimes \e_j^{\Vspace} \right)
\end{equation}
where $\r_i^{\Lmap} \in \Vspace$ is the $i$th 'row' of $\Lmap$
in the representation $\Lmap = \sum_{i=0}^{m-1} \e_i^{\Wspace} \otimes \r_i^{\Lmap}$.

The Jacobian, which has 4 indexes here, is given by:
\begin{equation}
\db{ij}{\LTL_{kl}}{\Lmap}
 =
\left( \db{ij}{\LTL}{\Lmap} \right)_{kl}
=
\delta_{jl} \Lmap_{ik}
+
\delta_{jk} \Lmap_{il}
\end{equation}
where, as usual, $\delta_{ij} = 1$ if $i=j$ and  $0$ if $i \neq j$.
From the Jacobian, we can compute the gradients of $\LTL_{kl}$
using equation \ref{eq:gradient-from-partials}
and the fact that
$Emap_{ij}^{\Lspace(\Vspace,\Wspace)}  = \e_i^{\Wspace} \otimes \e_j^{\Vspace}$
are the canonical basis vectors for $\Lspace(\Vspace,\Wspace)$:
\begin{eqnarray}
\Gb{\LTL_{kl}}{\Lmap}
& = &
\sum_{ij}
\left( \db{ij}{\LTL_{kl}}{\Lmap} \right)
\left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\\
& = &
\sum_{ij}
\left( \delta_{jl} \Lmap_{ik} + \delta_{jk} \Lmap_{il} \right)
\left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\nonumber
\\
& = &
\sum_{i}
\left(
\Lmap_{ik}  \e_i^{\Wspace} \otimes \e_l^{\Vspace}
\right)
+
\sum_{i}
\left(
\Lmap_{il}  \e_i^{\Wspace} \otimes \e_k^{\Vspace}
\right)
\nonumber
\\
& = &
\left(
\c_k^{\Lmap} \otimes \e_l^{\Vspace}
\right)
+
\left(
\c_l^{\Lmap} \otimes \e_k^{\Vspace}
\right)
\nonumber
\end{eqnarray}
where $\c_j^{\Lmap} \in \Wspace$ is the $j$th 'column' of $\Lmap$
in the representation
$\Lmap = \sum_{j=0}^{n-1} \c_j^{\Lmap} \otimes \e_j^{\Vspace}$.

%-----------------------------------------------------------------
\levelstay{Inverse}
\label{sec:Inverse}

$\Inverse()$ here is interpreted in the traditional sense:
$\Lmap^{-1}(\w) = \v$ if there exists a unique $\v$ such that $\w = \Lmap(\v)$,
and is either considered undefined, or assigned an arbitrary
value, such as $\0$, otherwise.
A function $\Lmap : \Vspace \mapsto \Wspace$ is \textit{invertible}
if, for all $\w \in \Wspace$, there exists a $\v$ such that
$\w = \Lmap \v$.
In any reasonable topology,
the set of invertible linear functions $\Vspace \mapsto \Wspace$
is an open subset of the set of all linear functions,
and $\Inverse()$ is continuous and differentiable there.

The partial derivative is the value of the following, when the limit exists:
\begin{displaymath}
\db{ij}{\Inverse()}{\Lmap}
 =
\lim_{ h \mapsto 0}
\frac{ \left( \Lmap + h (\e_i^{\Wspace} \otimes \e_j^{\Vspace}) \right)^{-1} - \Lmap^{-1} }{h}
\end{displaymath}
Note that
\begin{displaymath}
\Lmap + h (\e_i^{\Wspace} \otimes \e_j^{\Vspace})
 =
\left( \Identity^{\Wspace} - ( -h ( \e_i^{\Wspace} \otimes \e_j^{\Vspace} )) \Lmap^{-1} \right) \Lmap
\end{displaymath}
and
\begin{eqnarray*}
\left( \Lmap + h (\e_i^{\Wspace} \otimes \e_j^{\Vspace}) \right)^{-1}
& = &
\Lmap^{-1} \left( \Identity^{\Wspace} - ( -h )( \e_i^{\Wspace} \otimes \e_j^{\Vspace} ) \Lmap^{-1} \right)^{-1}
\\
& = &
\Lmap^{-1} \sum_{k=0}^{\infty} \left( -h ( \e_i^{\Wspace} \otimes \e_j^{\Vspace} ) \Lmap^{-1} \right)^{k}
\nonumber
\end{eqnarray*}
Therefore
\begin{displaymath}
\frac{ \left( \Lmap + h (\e_i^{\Wspace} \otimes \e_j^{\Vspace}) \right)^{-1} - \Lmap^{-1} }{h}
 =
- \Lmap^{-1} ( \e_i^{\Wspace} \otimes \e_j^{\Vspace} )  \Lmap^{-1} + O(h)
\end{displaymath}
which implies
\begin{equation}
\da{ij}{\Lmap^{-1}}
 =
- \left[
\Lmap^{-1}
\left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\Lmap^{-1}
\right]
\end{equation}

\newgeometry{onecolumn=true}

%-----------------------------------------------------------------
\levelstay{Pseudo-inverse}
\label{sec:Pseudo-inverse}

$\Pseudoinverse(\Lmap) \equiv \Lmap^{-}$

If $\kernel(\Lmap) = \0$, $\Lmap$ is said to have \textit{full rank}.
The set of full rank linear functions is an open set,
and we can define the derivative of $\Pseudoinverse(\Lmap)$ there.
For a full rank function,
$\Lmap^{-} = \left( \Lmap^{\dagger} \Lmap \right)^{-1} \Lmap^{\dagger}$
(see equation \ref{eq:full-rank-pseudo-inverse}).
It follows from equation \ref{eq:function-product-derivative} that
\begin{eqnarray}
\db{ij}{\Pseudoinverse}{\Lmap}
& = &
\db{ij}{\Inverse(\LTL())\Transpose()}{\Lmap}
\\
& = &
\left[
\left( \db{ij}{\Inverse(\LTL())}{\Lmap} \right)
\Lmap^{\dagger}
\right]
+
\left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\db{ij}{\Transpose()}{\Lmap}
\right]
\nonumber
\\
& = &
\left[
\left( \db{ij}{\Inverse(\LTL())}{\Lmap} \right)
\Lmap^{\dagger}
\right]
+
\left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_j^{\Vspace} \otimes \e_i^{\Wspace} \right)
\right]
\nonumber
\end{eqnarray}

By the chain rule
\begin{eqnarray}
\Db{\Inverse(\LTL())}{\Lmap}
& = &
\sum_{kl}
\db{kl}{\Inverse}{\Lmap^{\dagger}\Lmap}
\otimes
\Gb{\LTL_{kl}}{\Lmap}
\\
& = &
\sum_{kl}
- \left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_k^{\Vspace} \otimes \e_l^{\Vspace} \right)
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\right]
\otimes
\left[
\left( \c_k^{\Lmap} \otimes \e_l^{\Vspace} \right)
+
\left( \c_l^{\Lmap} \otimes \e_k^{\Vspace} \right)
\right]
\nonumber
\end{eqnarray}

To minimize confusion,
recall that $\Db{\Inverse(\LTL())}{\Lmap}$ is
a linear function from $\Lspace(\Vspace,\Wspace) \mapsto \Lspace(\Vspace,\Vspace)$.
Note that the central tensor product ($\otimes$) above
is a product of
$
- \left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_k^{\Vspace} \otimes \e_l^{\Vspace} \right)
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\right]
$,
an element of $\Lspace(\Vspace,\Vspace)$
and
$
\left[
\left( \c_k^{\Lmap} \otimes \e_l^{\Vspace} \right)
+
\left( \c_l^{\Lmap} \otimes \e_k^{\Vspace} \right)
\right]
$,
an element of $\Lspace(\Vspace,\Wspace)$.

It follows from equation \ref{eq:partial-full-dervatives} that
\begin{eqnarray}
\db{ij}{\Inverse(\LTL())}{\Lmap}
& = &
\Db{\Inverse(\LTL())}{\Lmap}
\left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\\
& = &
\sum_{kl}
- \left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_k^{\Vspace} \otimes \e_l^{\Vspace} \right)
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\right]
\otimes
\left[
\left( \c_k^{\Lmap} \otimes \e_l^{\Vspace} \right)
+
\left( \c_l^{\Lmap} \otimes \e_k^{\Vspace} \right)
\right]
\left( \e_i^{\Wspace} \otimes \e_j^{\Vspace} \right)
\nonumber
\\
& = &
\sum_{kl}
- \left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_k^{\Vspace} \otimes \e_l^{\Vspace} \right)
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\right]
\left[
\delta_{jl}
\Lmap_{ik}
+
\delta_{jk}
\Lmap_{il}
\right]
\nonumber
\\
& = &
-
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left[
\sum_{k}
\Lmap_{ik}
\left(
\left( \e_k^{\Vspace} \otimes \e_j^{\Vspace} \right)
+
\left( \e_j^{\Vspace} \otimes \e_k^{\Vspace} \right)
\right)
\right]
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\nonumber
\\
& = &
-
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left[
\left( \r_i^{\Lmap} \otimes \e_j^{\Vspace} \right)
+
\left( \e_j^{\Vspace} \otimes \r_i^{\Lmap} \right)
\right]
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\nonumber
\end{eqnarray}

Putting it all together:
\begin{eqnarray}
\db{ij}{\Pseudoinverse}{\Lmap}
& = &
\left[
-
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left[
\left( \r_i^{\Lmap} \otimes \e_j^{\Vspace} \right)
+
\left( \e_j^{\Vspace} \otimes \r_i^{\Lmap} \right)
\right]
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\Lmap^{\dagger}
\right]
+
\left[
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left( \e_j^{\Vspace} \otimes \e_i^{\Wspace} \right)
\right]
\nonumber
\\
& = &
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left[
\left( \e_j^{\Vspace} \otimes \e_i^{\Wspace} \right)
-
\left(
\left[
\left( \r_i^{\Lmap} \otimes \e_j^{\Vspace} \right)
+
\left( \e_j^{\Vspace} \otimes \r_i^{\Lmap} \right)
\right]
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\Lmap^{\dagger}
\right)
\right]
\nonumber
\\
& = &
\left( \Lmap^{\dagger} \Lmap \right)^{-1}
\left[
\left( \e_j^{\Vspace} \otimes \e_i^{\Wspace} \right)
-
\left(
\left( \r_i^{\Lmap} \otimes \e_j^{\Vspace} \right)
+
\left( \e_j^{\Vspace} \otimes \r_i^{\Lmap} \right)
\right)
\Lmap^{-}
\right]
\end{eqnarray}

\restoregeometry

