\chapter{Interpolation}\label{ch:Interpolation}

\cite{wiki:Interpolation,wiki:Polynomial-interpolation}

TODO:
\begin{itemize}
\item Better to way to characterize space of polynomials,
than just monomial form?
\item Polynomial form supporting updating algorithm when
 adding and droping constraints.
\item Basis functions supporting updating algorithm when
 adding and droping constraints --- any difference from above?
\item numerical accuracy by representation and degree?
\item sensitivity of interpolant values as function of constraint
 values, as function of degree of polynomial?
\item deriviative of argmin as function of knot location, value, 
 slope, etc., as function of degree.
 \item Tests for 'singular' interpolation problem in the sense 
 that not all coefficients are determined --- 
 for example, $3$ xy pairs that lie on a line. 
 \item Consistent naming for basis functions and coefficients?
\end{itemize}

Motivation: 
\begin{enumerate}
 \item Minimization of real-valued functions on linear spaces.
 \item Iterated line search methods.
 \item $1$d minimization of real-valued functions on a line,
 identified with \glssymbol{RealNumbers}.
 \item $1$d minimization methods based on visiting the
 argmin of a model function.
 \item model function constructed using values and derivatives
 of the $1$d objective function at recently visited locations.
 \item common model function are low degree polynomials 
 interpolating the supplied values and derivatives.
\end{enumerate}

$1$d minimization methods only use the argmin of the model 
function, so constructing a representation of the interpolating 
polynomial is usually unnecessary expense.
This code is intended to make it easy to either 
compute the argmin alone,
or reify the model function, for visualization and debugging.
(Maybe a lazy polynomial constructor that provides argmin with
minimum effort and defers other work until first call to 
\texttt{value} or \texttt{derivative}.)

The polynomials of degree $k$ for a linear space of dimension
$k+1$\cite{wiki:polynomial}.
The most common interpolating polynomials are quadratic or cubic,
so we will have $3$ or $4$ dimensions (degrees of freedom) to play
with. 
(I will discuss affine and constant interpolation for 
completeness.)

Why not higher order? Are other interpolating functions useful?
What about splines?

An interpolating polynomial matches some number of $x,y$
and $x,d$ pairs, where $x$ is a location in the domain,
$y$ a value in the codomain (range) and $d$ the slope.

The polynomials of degree $k$ for a linear space of dimension
$k+1$. 

A degree $k$ polynomial can match a combination of $k+1$ value 
and/or 
slope constraints, where there are at most $k$ slope constraints,
and where the value constraints are at distinct $x$'s and the
slope constraint are at distinct $x$'s, but a slope and a value 
constraint can be given at the same $x$.

A key issue is the choice of basis.

Some bases are may be more convenient when constructing a 
polynomial by interpolation.

Another consideration is the accuracy and cost of 
argmin \texttt{argmin}, \texttt{value}, and \texttt{derivative} 
when the interpolating polynomial is approximated with 
floating point numbers.

\texttt{BigFraction} implementation

Accuracy of evaluating the interpolating polynomial
versus how far the interpolating function is from the function it
interpolates.

Extrapolation often results when using the \texttt{argmin}.
Relationship between \texttt{argmin} and inverse interpolation.

\section{Monomial basis}\label{sec:Monomial-basis}

The most common basis is the monomial:
\begin{equation}
\mu(x) = \sum_{i=0}^{k} \mu_i x^i
\end{equation}

\textbf{TODO:} 
\begin{itemize}
  \item Evaluation speed?
  \item Horner's algorithm and fma
  \item differentiation/argmin advantage?
  \item numerical accuracy?
  \item easy to sum and multiply and divide within monomial
  representation
  \item degree obvious
\end{itemize}

(Note: these basis function aren't orthogonal or normalized)
in any
sense. In fact, choosing a distance or an inner product for
a polynomial space is not a simple question.)

Constructing an interpolating polynomial in the monomial basis
requires solving a dense system of equations. 
No advantage for small changes to constraints.

Suppose, for example, we have $x_0,y_0,d_0$ and
$x_1,y_1,d_1$, that is, we have specified the value and slope we
want at $x_0 \neq x_1$.
That's $4$ constraints, impying a cubic interpolant.
To determine the monomial basis coefficients,
solve the linear system:

\begin{equation}
\begin{pmatrix}
y_0 \\ d_0 \\ y_1 \\ d_1
\end{pmatrix}
=
\begin{pmatrix}
1 & x_0 & \phantom{2} x_0^2 & \phantom{3} x_0^3 \\
  & 1   & 2 x_0 & 3 x_0^2 \\
1 & x_1 & \phantom{2} x_1^2 & \phantom{3} x_1^3 \\
  & 1   & 2 x_1 & 3 x_1^2 
\end{pmatrix}
\begin{pmatrix}
\mu_0 \\ \mu_1 \\ \mu_2 \\ \mu_3
\end{pmatrix}
\end{equation}

\begin{align}
 y_0 & = \mu_0 + \mu_1 x_0 + \phantom{2} \mu_2 x_0^2 + \phantom{3} \mu_3 x_0^3 \\
 d_0 & = \phantom{\mu_0} \phantom{+} \mu_1 \phantom{x_0} + 2 \mu_2 x_0 + 3 \mu_3 x_0^2 \nonumber \\
 y_1 & = \mu_0 + \mu_1 x_1 + \phantom{2} \mu_2 x_1^2 + \phantom{3} \mu_3 x_1^3 \nonumber \\
 d_1 & = \phantom{\mu_0} \phantom{+} \mu_1 \phantom{x_0} + 2 \mu_2 x_1 + 3 \mu_3 x_1^2 \nonumber
\end{align}

\begin{align}
 y_0 = & {\mu_0 + \mu_1 x_0 + \phantom{2} \mu_2 x_0^2 + \phantom{3} \mu_3 x_0^3} \\
 d_0 = & \pushright{\mu_1 \phantom{x_0} + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = & {\mu_0 + \mu_1 x_1 + \phantom{2} \mu_2 x_1^2 + \phantom{3} \mu_3 x_1^3 }\nonumber \\
 d_1 = & \pushright{\mu_1 \phantom{x_0} + 2 \mu_2 x_1 + 3 \mu_3 x_1^2} \nonumber
\end{align}

\begin{align}
 y_0 = & {\mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3} \\
 d_0 = & \pushright{\mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = & {\mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 }\nonumber \\
 d_1 = & \pushright{\mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2} \nonumber
\end{align}

\begin{align}\label{eq:hermite-eqns}
 y_0 & = \mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3 \\
 d_0 & = \mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2 \nonumber \\
 y_1 & = \mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 \nonumber \\
 d_1 & = \mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2 \nonumber 
\end{align}

\begin{align}
 y_0 = &\mu_0 + \mu_1 x_0 + \mu_2 x_0^2 + \mu_3 x_0^3 \\
 d_0 = &\specialcell{\hfill \mu_1 + 2 \mu_2 x_0 + 3 \mu_3 x_0^2} \nonumber \\
 y_1 = &\mu_0 + \mu_1 x_1 + \mu_2 x_1^2 + \mu_3 x_1^3 \nonumber \\
 d_1 = &\specialcell{{\hfill \mu_1 + 2 \mu_2 x_1 + 3 \mu_3 x_1^2}} \nonumber 
\end{align}

\subsection{Differentiation and \texttt{argmin}}

Degree $k$ polynomial:
\begin{align}
\mu(x) & = \sum_{i=0}^{k} \mu_i x^i
\\
\partial{\mu}(x) & = \sum_{i=1}^{k} i \mu_i x^{i-1}
\nonumber
\\
\partial^2{\mu}(x) & = \sum_{i=2}^{k} i (i-1) \mu_i x^{i-2}
\nonumber
\end{align}

Critical points $\hat{x}$ occur where 
$ 0 = \partial{\mu}(\hat{x}) $,
with $\hat{x}$ is a local minimum if 
$ 0 < \partial^2{\mu}(\hat{x}) $.
It can be helpful to look at how $\hat{x}$ depends
on the parameters of the polynomial representation,
and on the inputs to interpolation that determine those 
parameters.

\subsection{Constant Monomial}

\begin{equation}
\mu(x) = \mu_0
\end{equation}

Constant monomials have no critical points or local/global
minima. 
(So implementation returns \texttt{NaN} for \texttt{(argmin mu)}).

The only possibility is to 'interpolate' $(x_0,y_0)$ with
$\mu_0 = y_0$ (and $\mu_i = 0$ for other $i$). 

\subsection{Affine Monomial}

\begin{equation}
\mu(x) = \mu_0 + \mu_1 x
\end{equation}

Affine monomials have no critical points.
The global minimum/maximum is at $\mp \sign(\mu_1) \infty$.

For interpolation, there are $2$ possibilities, finding the line thru $2$
points, $(x_0,y_0)$ and $(x_1,y_1)$, or or matching the value and slope at
$(x_0,y_0,d_0)$.

\subsubsection{$(x_0,y_0), (x_1,y_1)$}

\input{monomial-yy}

\subsubsection{$(x_0,y_0,d_0)$}

\input{monomial-yd}

\subsection{Quadratic Monomial}

\begin{equation}
\mu(x) = \mu_0 + \mu_1 x + \mu_2 x^2
\end{equation}

Critical point at
\begin{equation}
\hat{x} = \frac{-\mu_1}{2 \mu_2}
\end{equation}
$\hat{x}$ is the global minimum if $\mu_2>0$ 
and the global maximum if $\mu_2>0$.
If $\mu_2 = 0$, the global maximum is at 
$\text{sign}(\mu_1)\infty$
and the minimum is at $-\text{sign}(\mu_1)\infty$.

Dependence on monomial coeficients:
\begin{equation}
\nabla_{\vec{\mu}} \hat{x} =
\begin{pmatrix}
0 
\\
- \frac{1}{2 \mu_2} 
\\
\frac{\mu_1}{\mu_2^2}
\end{pmatrix}
\end{equation}

\subsubsection{yyy}

\input{monomial-yyy}

\subsubsection{yyd}

$3$ domain locations:
\input{monomial-yyd3}

$2$ domain locations:
\input{monomial-yyd2}

\subsubsection{ydd}

$3$ domain locations:
\input{monomial-ydd3}

$2$ domain locations (essentially the secant step):
\input{monomial-ydd2}

\subsection{Cubic Monomial}

\begin{equation}
\mu(x) = \mu_0 + \mu_1 x + \mu_2 x^2 + \mu_3 x^3
\end{equation}

Critical points are at
\begin{equation}
\hat{x}_{\pm} = \frac{-\mu_2 \pm \sqrt{ \mu_2^{2} - 3 \mu_1 \mu_3 }}{3 \mu_3}
\end{equation}
when $\mu_2^{2} > 3 \mu_1 \mu_3$.
One point is a local minimum, one a local maximum, 
determined by the signs of of the second derivatives
$\partial^2\mu(\hat{x}_{\pm}) = 2 \mu_2 + 6 \mu_3 \hat{x}_{\pm}$.

If $\mu_2^{2} = 3 \mu_1 \mu_3$, 
then $\hat{x} = \hat{x}_{+} = \hat{x}_{-}$ is just a stationary
point, neither a local minimum or maximum.

If $\mu_2^{2} < 3 \mu_1 \mu_3$, then there are no critical points,
no local minima/maxima, and the global minimum (maximum) is at
$\mp\text{sign}(\mu_3)\infty$.

Dependence on monomial coeficients:
\begin{equation}
\nabla_{\vec{\mu}} \hat{x} =
\begin{pmatrix}
0 
\\
\dfrac{
\mp \mu_3^{2}
}{
2 \sqrt{\mu_2^{2} -3 \mu_1 \mu_3}
}
\\
\dfrac{
-\mu_3 \left(\sqrt{\mu_2^{2} -3 \mu_1 \mu_3} \mp \mu_2\right)
}{
3 \sqrt{\mu_2^{2} -3 \mu_1 \mu_3}
}
\\
\dfrac{
\mp \mu_1 \mu_3
+ 2 \sqrt{\mu_2^{2} - 3 \mu_1 \mu_3}
\left(\pm \sqrt{\mu_2^{2} - 3 \mu_1 \mu_3}  - \mu_2 \right)
}{
6 \sqrt{\mu_2^{2} -3 \mu_1 \mu_3}
}
\end{pmatrix}
\end{equation}

\newgeometry{onecolumn=true}

\subsubsection{yyyy}

\input{monomial-yyyy-1}

% \subsubsection{yyyd}
% 
% $4$ domain locations:
% \input{monomial-yyyd4}
% 
% $3$ domain locations:
% \input{monomial-yyyd3}

\subsubsection{yydd}

Showing the most likely cases.

% $4$ domain locations:
% \input{monomial-yydd4}

$2$ domain locations:
\input{monomial-yydd2}

% \subsubsection{yddd}
% $4$ domain locations:
% 
% \input{monomial-yddd4}

\restoregeometry
  
\section{Lagrange basis}\label{sec:Lagrange-basis}

The standard Lagrange\cite{wiki:Lagrange-polynomial}
form of a polynomial  is
\begin{equation}
f(x) = \sum_{i} y_i \prod_{j \neq i} \frac{x - x_j}{x_i -x_j}
\end{equation}

We can interpret this as a linear combination of basis functions,
$\lambda_i (x) = \prod_{j \neq i} \frac{x - x_j}{x_i -x_j}$.
Each $\lambda_i$ is $1$ at $x_i$ and $0$ at all the other $x_j$.
So we can use $\lambda_i$ to satisfy a value constraint at $x_i$
without interfering with value constraints at the other $x_j$.

That suggests that it might be useful to construct similar basis
functions to interpolate combinations of value and slope 
constraints.

\textbf{TODO:} 
\begin{itemize}
  \item Can all real polynomials be represented in this form? 
        With addition of constants? Maybe not, since some
        quadratics can't be factored over reals.
  \item Evaluation speed?
  \item Horner's algorithm and fma
  \item differentiation/argmin disadvantage?
  \item numerical accuracy?
  \item easy to sum and multiply and divide within monomial
  representation
\end{itemize}

(Note: these basis function aren't orthogonal or normalized)
in any
sense. In fact, choosing a distance or an inner product for
a polynomial space is not a simple question.)

Constructing an interpolating polynomial in the monomial basis
requires solving a dense system of equations. 
No advantage for small changes to constraints.

\subsection{Differentiation and \texttt{argmin}}

\subsection{Affine Lagrange}

\subsubsection{yy}

It's easy enough to see that:
\begin{align}
\lambda^{\text{yy}}_0(x) & = \frac {(x - x_1)} {(x_0 - x_1)} \\
\lambda^{\text{yy}}_1(x) & = \frac {(x - x_0)} {(x_1 - x_0)} \nonumber
\end{align}
satisfies
\begin{align}
\lambda^{\text{yy}}_0(x_0) & = 1 \\
\lambda^{\text{yy}}_0(x_1) & = 0 \nonumber \\
\lambda^{\text{yy}}_1(x_0) & = 0 \nonumber \\
\lambda^{\text{yy}}_1(x_1) & = 1 \nonumber 
\end{align}


\subsubsection{yd}

We need $\partial\lambda^{\text{yd}}_0 (x_1) = 0$,
but that implies affine $\lambda^{\text{yd}}_0$ 
must be a constant.
So we have to extend our definition of Lagrange basis functions
to include constants.
We also need $\lambda^{\text{yd}}_1 (x_0) = 0$ and
$\partial\lambda^{\text{yd}}_1 (x_1) = 1$, leading to the basis:
\begin{align}
\lambda^{\text{yd}}_0 (x) & = 1 \\
\lambda^{\text{yd}}_1 (x) & = x - x_0 \nonumber
\end{align}


\subsection{Quadratic Lagrange}

\subsubsection{yyy}

For example:

\begin{equation}
\lambda^{\text{yyy}}_0(x) = 
\frac {(x - x_1) (x - x_2)} {(x_0 - x_1) (x_0 - x_2)}
\end{equation}

\subsubsection{yyd}

We need 
$\lambda^{\text{yyd}}_0(x_0) = 1$, 
$\lambda^{\text{yyd}}_0(x_1) = 0$, and
$\partial\lambda^{\text{yyd}}_0(x_2) = 0$. 

The second constraint suggests looking at functions of the form
$f(x) = (x - x_1) (x - c)$, which includes all quadratic 
polynomials that are zero at $x_1$, up to a scaling factor.  
$\partial{f}(x) = (x - x_1) + (x - c) = 2 x - x_1 - c$.
$\partial{f}(x_2) = 0$ implies that
$c = 2 x_2 - x_1$,
so
\begin{equation}
\lambda^{\text{yyd}}_0(x) = 
\frac 
{(x - x_1) \left( (x - x_2) + (x_1 - x_2) \right)} 
{(x_0 - x_1) ((x_0 - x_2) + (x_1 - x_2))}
\end{equation}
Note that we may have $x_0 = x_2$ or $x_1 = x_2$, but not both,
and not $x_0 = x_1$ --- otherwise it would be impossible to 
satisfy the constraints.

By symmetry, 
\begin{equation}
\lambda^{\text{yyd}}_1(x) = 
\frac 
{(x - x_0) \left( (x - x_2) + (x_0 - x_2) \right)} 
{(x_1 - x_0) \left( (x_1 - x_2) + (x_0 - x_2) \right)}
\end{equation}

 For $\lambda^{\text{yyd}}_2$, we need 
$\lambda^{\text{yyd}}_2(x_0) = 0$, 
$\lambda^{\text{yyd}}_2(x_1) = 0$, and
$\partial\lambda^{\text{yyd}}_2(x_2) = 1$. 
The standard form $f(x) = (x-x_0) (x-x_1)$ satisfies the first 
$2$ constraints. 
To satisfy the third, we simply need to normailize with
$\partial{f}(x_2) = (x_2 - x_0) + (x_2 - x_1)$:

\begin{equation}
\lambda^{\text{yyd}}_2(x) = 
\frac 
{(x - x_0) (x - x_1)} 
{(x_2 - x_0) + (x_2 - x_1)}
\end{equation}

\subsubsection{ydd}\label{sec:lagrange-ydd}

(Secant)

For $\lambda^{\text{ydd}}_0$ we need:
\begin{align}
\lambda^{\text{ydd}}_0(x_0) & = 1 \\
\partial\lambda^{\text{ydd}}_0(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{ydd}}_0(x_2) & = 0 \nonumber
\end{align}
Because $\lambda^{\text{ydd}}_0$ is quadratic,
$\partial\lambda^{\text{ydd}}_0$ is affine, 
and the only way its values
at $x_1$ and $x_2$ can both be zero is if it is a constant, so 
\begin{equation}
\lambda^{\text{ydd}}_0(x) = 1
\end{equation}

$\lambda^{\text{ydd}}_1$ must satisfy:
\begin{align}
\lambda^{\text{ydd}}_1(x_0) & = 0 \\
\partial\lambda^{\text{ydd}}_1(x_1) & = 1 \nonumber \\
\partial\lambda^{\text{ydd}}_1(x_2) & = 0 \nonumber
\end{align}
This suggests functions proportional to $(x - x_0) (x - c)$ 
for some $c$. Then 
$ 0 = (x_2 - x_0) + (x_2 - c)$
implies $ c = 2 x_2 - x_0$.
Then we normalize with 
$\partial\lambda^{\text{ydd}}_1(x_1) = (x_1 - x_0) + (x_1 - x_2) + (x_0 - x2) 
= 2 (x_1 - x_2)$, so 
\begin{equation}
\lambda^{\text{ydd}}_1(x) = 
\frac {(x - x_0) \left((x - x_2) + (x_0 - x_2)\right)} 
{2 (x_1 - x_2)}
\end{equation}
and, by symmetry,
\begin{equation}
\lambda^{\text{ydd}}_2(x) = 
\frac {(x - x_0) \left( (x -x_1) + (x_0 - x_1) \right)} 
{2 (x_2 - x_1)}
\end{equation}

\subsection{Cubic Lagrange}
 
\subsubsection{yyyy}

For example:

\begin{equation}
\lambda^{\text{yyy}}_0(x) = 
\frac {(x - x_1) (x - x_2) (x - x_3)} 
{(x_0 - x_1) (x_0 - x_2) (x_0 - x_3)}
\end{equation}

% \subsubsection{yyyd}
% 
% We need 
% \begin{align}
% \lambda^{\text{yyyd}}_0(x_0) & = 1 \\ 
% \lambda^{\text{yyyd}}_0(x_1) & = 0 \nonumber \\
% \lambda^{\text{yyyd}}_0(x_2) & = 0 \nonumber \\
% \partial\lambda^{\text{yyyd}}_0(x_3) & = 0 \nonumber 
% \end{align}
% 
% Consider functions proportional to $f(x) = (x-x_1)(x-x_2)(x-c_0)$.
% This satisfies the first $3$ constraints.
% For the fourth, we need
% \begin{equation}
% 0 = (x_3-x_1)(x_3-x_2) + (x_3-x_2)(x_3-c_0) + (x_3-c_0)(x_3-x_1)
% \end{equation}
% which implies
% \begin{align}
% c_0 & = 
% \frac
% {x_3\left((x_3-x_1)+ (x_3-x_2)\right) + (x_3-x_1)(x_3-x_2)}
% {(x_3-x_1)+ (x_3-x_2)} 
% \\
%  &= x_3 + 
% \frac
% {(x_3-x_1)(x_3-x_2)}
% {(x_3-x_1)+ (x_3-x_2)} 
% \nonumber
% \end{align}
% 
% To satisfy the first constraint,
% we normalize with $f(x_0)$.
% Therefore:
% \begin{align}
% \lambda^{\text{yyyd}}_0(x) & = 
% \frac {(x - x_1) (x - x_2) (x - c_0)} 
% {(x_0 - x_1) (x_0 - x_2) (x_0 - c_0))} 
% \\
% \lambda^{\text{yyyd}}_1(x) & = 
% \frac {(x - x_2) (x - x_0) (x - c_1)} 
% {(x_1 - x_2) (x_1 - x_0) (x_1 - c_1))} 
% \nonumber \\
% \lambda^{\text{yyyd}}_2(x) & = 
% \frac {(x - x_0) (x - x_1) (x - c_2)} 
% {(x_2 - x_0) (x_2 - x_1) (x_2 - c_2))} 
% \nonumber \\
% \lambda^{\text{yyyd}}_3(x) & = 
% \frac {(x - x_0) (x - x_1) (x - x_2)} 
% {\left( (x_3 - x_0) (x_3 - x_1) \right)
% + \left( (x_3 - x_1) (x_3 - x_2) \right)
% + \left( (x_3 - x_2) (x_3 - x_0) \right)} 
% \nonumber
% \end{align}
% 
% $\lambda^{\text{yyyd}}_1$ and $\lambda^{\text{yyyd}}_2$ follow by symmetry.
% $\lambda^{\text{yyyd}}_3$ is the standard Lagrange form,
% normalized for unit slope rather unit value, and fairly obviously
% satisfies its constraintz.
 
\subsubsection{yydd}\label{sec:lagrange-yydd}

We need 
\begin{align}
\lambda^{\text{yydd}}_0(x_0) & = 1 \\ 
\lambda^{\text{yydd}}_0(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_0(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_0(x_3) & = 0 \nonumber 
\end{align}

Consider functions of the form $(x-x_1)(x-(a_0+b_0))(x-(a_0-b_)))$.
The second (value) constraint is automatically satisfied.
The slope constraints are satisfied by
% \begin{align}
% a_0 & = \frac{3 (x_2 + x_3) - 2 x_1}{4} \\ 
% b_0 & = \frac{
% \sqrt{3}
% \sqrt{
% - 4 x_1^{2} 
% + 4 x_1 x_2 
% + 4 x_1 x_3 
% + 3 x_2^{2}
% - 10 x_2 x_3
% + 3 x_3^{2}
% }
% }
% {4} \nonumber
% \end{align}
\begin{align}
a_0 & = \frac{3 (x_2 + x_3) - 2 x_1}{4} \\ 
b_0 & = \frac{
\sqrt{ 9 (x_2 - x_3)^2 - 12 (x_1 - x_2) (x_1 -x_3)}
}
{4} \nonumber
\end{align}
(See \texttt{lagrange-yydd.reduce} for an example of using a
symbolic algebra system to solve this.)

By symmetry,  
\begin{align}
\lambda^{\text{yydd}}_0(x) & =
\frac{(x-x_1)(x-(a_0+b_0))(x-(a_0-b_0))}
{(x_0-x_1)(x_0-(a_0+b_0))(x_0-(a_0-b_0))}
\\
\lambda^{\text{yydd}}_1(x) & =
\frac{(x-x_0)(x-(a_1+b_1))(x-(a_1-b_1))}
{(x_1-x_0)(x_1-(a_0+b_1))(x_1-(a_1-b_1))}
\nonumber
\end{align}
where $a_1$ and $b_1$ can be obtained 
by replacing $x_1$ by $x_0$ in the formulas
above.

For $\lambda^{\text{yydd}}_2$ and $\lambda^{\text{yydd}}_3$, 
consider functions of the form $(x-x_0)(x-x_1)(x-c_i)$.
We need, for example, 
\begin{align}
\lambda^{\text{yydd}}_3(x_0) & = 1 \\ 
\lambda^{\text{yydd}}_3(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_3(x_2) & = 0 \nonumber \\
\partial\lambda^{\text{yydd}}_3(x_3) & = 1 \nonumber 
\end{align}
It's reasonably easy to see that 
\begin{align}
c_2 & = x_3 + 
\frac{(x_3 - x_0)(x_3 - x_1)}{(x_3 - x_0) + (x_3 - x_1)} \\
c_3 & = x_2 + 
\frac{(x_2 - x_0)(x_2 - x_1)}{(x_2 - x_0) + (x_2 - x_1)} \\
\end{align}
so
\begin{align}
\lambda^{\text{yydd}}_2(x) & =
\frac{(x-x_0)(x-x_1)(x-c_2)}
{
(x_2-x_0)(x_2-x_1) +
(x_2-x_1)(x_2-c_2) +
(x_2-c_2)(x_2-x_0)}
\\
\lambda^{\text{yydd}}_3(x) & =
\frac{(x-x_0)(x-x_1)(x-x_2)}
{
(x_3-x_0)(x_3-x_1) +
(x_3-x_1)(x_3-c_3) +
(x+3-c_3)(x_3-x_0)}
\nonumber
\end{align}

\paragraph{Hermite interpolation}\label{sec:Hermite-yydd}

When $x_0 = x_2$ and $x_1 = x_3$ --- $2$ knots with value and 
slope specified at each --- 
we have the standard cubic Hermite interpolation
problem\cite{wiki:cubic-hermite-spline}.

The constraints are then:
\begin{align}
\lambda^{\text{hermite}}_{00}(x_0) & = 1 \\ 
\lambda^{\text{hermite}}_{00}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{00}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{00}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{01}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{01}(x_1) & = 1 \nonumber \\
\partial\lambda^{\text{hermite}}_{01}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{01}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{10}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{10}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{10}(x_0) & = 1 \nonumber \\
\partial\lambda^{\text{hermite}}_{10}(x_1) & = 0 \nonumber \\
\lambda^{\text{hermite}}_{11}(x_0) & = 0 \\ 
\lambda^{\text{hermite}}_{11}(x_1) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{11}(x_0) & = 0 \nonumber \\
\partial\lambda^{\text{hermite}}_{11}(x_1) & = 1 \nonumber 
\end{align}
The $4$ basis functions can be determined by substituting $x_0$
for $x_2$ and $x_1$ for $x_3$ in the results in 
section~\ref{sec:lagrange-yydd}:
\begin{align}
\lambda^{\text{hermite}}_{00}(x) & =
\frac
{(x-x_1)^2 \left(2(x - x_0) + (x_1 - x_0) \right)}
{(x_1-x_0)^3}
\\
\lambda^{\text{hermite}}_{01}(x) & =
\frac
{(x-x_0)^2 \left(2(x - x_1) + (x_0 - x_1) \right)}
{(x_0-x_1)^3}
\nonumber \\
\lambda^{\text{hermite}}_{10}(x) & =
\frac {(x-x_1)^2(x-x_0)} {(x_0-x_1)^2}
\nonumber \\
\lambda^{\text{hermite}}_{11}(x) & =
\frac {(x-x_0)^2(x-x_1)} {(x_1-x_0)^2}
\nonumber
\end{align}

% \subsubsection{yddd}\label{sec:lagrange-yddd}
% 
% For $\lambda^{\text{yddd}}_0$, we need 
% \begin{align}
% \lambda^{\text{yddd}}_0(x_0) & = 1 \\ 
% \partial\lambda^{\text{yddd}}_0(x_1) & = 0 \nonumber \\
% \partial\lambda^{\text{yddd}}_0(x_2) & = 0 \nonumber \\
% \partial\lambda^{\text{yddd}}_0(x_3) & = 0 \nonumber 
% \end{align}
% As in section~\ref{sec:lagrange-ydd},
% this can only be satisfied by a constant function:
% $\lambda^{\text{yddd}}_0(x) = 1$.
% 
% The other $3$ basis functions must satisfy variations of
% \begin{align}
% \lambda^{\text{yddd}}_1(x_0) & = 0 \\ 
% \partial\lambda^{\text{yddd}}_1(x_1) & = 1 \nonumber \\
% \partial\lambda^{\text{yddd}}_1(x_2) & = 0 \nonumber \\
% \partial\lambda^{\text{yddd}}_1(x_3) & = 0 \nonumber 
% \end{align}
% This is essentially the same problem as that of $\lambda^{\text{yydd}}_0$
% in section~\ref{sec:lagrange-yydd},
% so the solution is:
% \begin{align}
% \lambda^{\text{yddd}}_1(x) & =
% \frac{(x-x_0)(x-(a_1+b_1))(x-(a_1-b_1))}
% {(x_1-x_0)(x_1-(a_1+b_1))(x_1-(a_1-b_1))}
% \\
% \lambda^{\text{yddd}}_2(x) & =
% \frac{(x-x_0)(x-(a_2+b_2))(x-(a_2-b_2))}
% {(x_2-x_0)(x_2-(a_2+b_2))(x_2-(a_2-b_2))}
% \nonumber \\
% \lambda^{\text{yddd}}_3(x) & =
% \frac{(x-x_0)(x-(a_3+b_3))(x-(a_3-b_3))}
% {(x_3-x_0)(x_3-(a_3+b_3))(x_3-(a_3-b_3))}
% \nonumber
% \end{align}
% where
% \begin{align}
% a_1 & = 
% \frac{3 (x_2 + x_3) - 2 x_0}{4} 
% \\ 
% b_1 & = 
% \frac{\sqrt{ 9 (x_2 - x_3)^2 - 12 (x_0 - x_2) (x_0 -x_3)}}{4} 
% \nonumber
% \end{align}
% and similarly for $a_2,b_2$ and $a_3,b_3$.
 
\section{Newton basis}
 \cite{wiki:Newton-polynomial}

One advantage of the Lagrange basis 
(section~\ref{sec:Lagrange-basis})
over the monomial (section~\ref{sec:Monomial-basis}),
besides giving a simpler system of equations to solve initially,
is the fact that the basis functions depend only on the locations
of the knots, and not the values or slopes specified at those 
points. 
That makes it easy to update the interpolant 
(though not its argmin) if we change the specified values
or slopes.
 
However, it doesn't help if we want add and remove knots,
as is the case in one-dimensional search applications.

The Newton basis supports adding knots, increasing the degree
of the interpolating polynomial with each added constraint.
So it, too, is of limited value for our driving application.

General form interpolating 
$(x_{0},y_{0}),\ldots ,(x_{j},y_{j}),\ldots ,(x_{k},y_{k})$:

\begin{equation}
\nu(x) = \sum _{{j=0}}^{{k}}a_{{j}} {\beta}_{{j}}(x)
\end{equation}
where the basis functions are similar to the Lagrange:
\begin{equation}
{\beta}_{j}(x) \defeq \prod_{{i=0}}^{{j-1}}(x-x_{i})
\end{equation}
but the coefficients are defined using 
\textit{divided differences} 
(section~\ref{sec:Divided-differences}):
\begin{equation}
a_{j} \defeq [y_{0},\ldots ,y_{j}] 
\end{equation}

Note that adding an additonal constraint
adds anew basis function and term in the sum, without affecting 
any of the previous ones.
Because we are only interested at most cubic interpolants,
and delete constraints as often as we add them,
this property has limited value.

As with the Lagrange basis, we can extend the usual definition
to handle slope constraints as well as value constraints.
In this case, however, it's not feasible to work through
all the quadratic and cubic cases, since the result depends
on the order in which constraints are specified, not just
how many of each kind there are.

\subsection{Quadratic: $(x_0,y_0),(x_1,y_1,d_1)$}

Quadratic case where value is specified at $x_0$ and $x_1$,
and slope is also specified at $x_1$.
General quadratic function in terms of Newton basis at
$x_0, x_1$:
\begin{align}
\nu(x) & = \nu_0 + \nu_1 (x - x_0) + \nu_2 (x - x_0) (x - x_1) 
\\
\partial{\nu}(x) & = \nu_1 + \nu_2 \left( (x - x_0) + (x - x_1) \right)
\nonumber
\end{align}

\begin{align}
y_0 = \nu(x_0) = \nu_0 & \Rightarrow \nu_0 = y_0 
\\
y_1 = \nu(x_1) = \nu_0 + \nu_1 (x_1 - x_0) & \Rightarrow 
\nu_1 = \frac{y_1-y_0}{x_1 - x_0} 
\nonumber \\
d_1 = \partial{\nu}(x_1) = \nu_1 + \nu_2 (x_1 - x_0) & \Rightarrow 
\nu_2 = \frac{d_1 (x_1 - x_0) - (y_1-y_0)}{(x_1 - x_0)^2} 
\nonumber
\end{align}

In this case, the critical point is at
\begin{equation}
x = x_0 + \frac{x_{01} y_{01}}{d_1 x_{01} - y_{01}}
\end{equation}
This is a global minimum if 
$\nu_2 = \frac{y_{01} - d_1 x_{01}}{x_{01}^2}$
is positive.

\subsection{Cubic: $(x_0,y_0,d_0),(x_1,y_1,d_1)$}

Cubic (Hermite~\cite{wiki:cubic-hermite-spline}) 
case where value and slope are specified at $x_0$ and $x_1$.
General cubic function in terms of Newton basis at
$x_0, x_1$:
\begin{align}
\nu(x) & = \nu_0 + \nu_1 x_{{\star}0} + \nu_2 x_{{\star}0} x_{{\star}1} 
 + \nu_3 x_{{\star}0} x_{{\star}1} x_{{\star}2} \\
\partial{\nu}(x) & = \nu_1 
+ \nu_2 \left( x_{{\star}0} + x_{{\star}1} \right) 
+ \nu_3 \left( 
 x_{{\star}0} x_{{\star}1} +
 x_{{\star}1} x_{{\star}2} +
 x_{{\star}2} x_{{\star}0} +
\right)
\nonumber
\end{align}
where $x_{{\star}i} = \left(x - x_i\right)$.

The interpolation constraints are:
\begin{align}
y_0 & = \nu_0 
\\
y_1 & = \nu_0 + \nu_1 x_{10}  
\nonumber 
\\
d_0 & = \nu_1 + \nu_2 x_{01} - \nu_3 x_{01} x_{20} 
\\
d_1 & = \nu_1 - \nu_2 x_{01} - \nu_3 x_{01} x_{12} 
\nonumber
\end{align}
(using $x_{ij} = - x_{ji}$).

We have $4$ constraints but $5$ unknowns, counting $x_2$
as unspecified.
If we take $x_2 = \frac{x_0 + x_1}{2}$,
then $x_{20} = x_{12} = -\frac{x_{01}}{2}$,
and the constraints reduce to
\begin{align}
y_0 & = \nu_0 
\\
y_1 & = \nu_0 + \nu_1 x_{10}  
\nonumber 
\\
d_0 & = \nu_1 + \nu_2 x_{01} + \nu_3 \frac{x_{01}^2}{2} 
\\
d_1 & = \nu_1 - \nu_2 x_{01} + \nu_3 \frac{x_{01}^2}{2} 
\nonumber
\end{align}

(\textbf{TODO:} Is this the best choice for $x_2$? 
If so, is there a good argument why?)

The Newton coefficients are then
\begin{align}
\nu_0 & = y_0 
\\
\nu_1 & = \frac{y_{01}}{x_{01}}
\nonumber
\\ 
\nu_2 & = \frac{d_{01}}{2 x_{01}}
\nonumber
\\ 
\nu_3 & = \frac{ \left(d_0 + d_1 \right) x_{01} + 2 y_{01}}{x_{01}^3}
\nonumber
\end{align} 

In terms of those coefficients, the critical points are at
\begin{equation}
x=\frac{-2 \nu_2 + 3 \nu_3 \left( x_0 + x_1 \right) 
\pm \sqrt{-12 \nu_1 \nu_3 + 4 \nu_2^{2} + 3 \nu_3^{2} x_{01}^{2}}
}
{ 6 \nu_3}
\end{equation}
when the argument of the square root is non-negative.
One of these is a local minimum when the $2$nd derivative is positive:
\begin{equation}
\partial^2 \nu(x) = 2\nu_2
 + 2 \nu_3 \left(x_{{\star}0} + x_{{\star}1} + x_{{\star}2}\right)
\end{equation}


\newpage
\section{Divided differences}\label{sec:Divided-differences}
 \cite{wiki:Divided-differences}
Given $k+1$ pairs 
$\left( x_0, y_0 \right) \ldots \left( x_k, y_k \right) $:

For $0 \leq \nu \leq k$:

\begin{align}
\left[ y_{\nu} \right] & \defeq y_{\nu}
\\
\left[ y_{\nu}, \ldots , y_{\nu + j} \right] & \defeq 
\frac{
\left[ y_{\nu + 1}, \ldots ,  y_{\nu + j} \right]
-
\left[ y_{\nu}, \ldots ,  y_{\nu + j - 1} \right]
}{
x_{\nu + j} - x_{\nu}
} 
\; \text{(forward)}
\nonumber
\\
\left[ y_{\nu}, \ldots , y_{\nu - j} \right] & \defeq 
\frac{
\left[ y_{\nu}, \ldots ,  y_{\nu - j + 1} \right]
-
\left[ y_{\nu - 1}, \ldots ,  y_{\nu - j} \right]
}{
x_{\nu} - x_{\nu - j}
} 
\; \text{(backward)}
\nonumber
\end{align}

Note: $x_i$ are implied in the standard 
$\left[ y_{\nu}, \ldots , y_{\nu + j} \right]$ notation.

Given $x_0, \ldots , x_k$ and $f$:

For $0 \leq \nu \leq k$:

\begin{align}
f \left[ x_{\nu} \right] & \defeq f \left( x_{\nu} \right)
\\
f \left[ x_{\nu}, \ldots , x_{\nu + j} \right] & \defeq 
\frac{
f \left[ x_{\nu + 1}, \ldots ,  x_{\nu + j} \right]
-
f \left[ x_{\nu}, \ldots ,  x_{\nu + j - 1} \right]
}{
x_{\nu + j} - x_{\nu}
} 
\nonumber
\end{align}

Relevant special cases ($z_{ij} \defeq z_i - z_j$ for $z$ 
either $x$ or $y$): 
\begin{align}
\left[ y_0 \right] & = y_0 
\\
\left[ y_0 , y_1  \right] & = 
\dfrac{y_{01}}{x_{01}} 
\nonumber
\\
\left[ y_0 , y_1 , y_2 \right] & = 
\dfrac{
\left( \dfrac{y_{12}}{x_{12}} - \dfrac{y_{01}}{x_{01}} \right)
}{
x_{20}
} 
\nonumber
\\
\left[ y_0 , y_1 , y_2 , y_3 \right] & = 
\dfrac{
 \left(
 \dfrac{
 \left(
  \frac{y_{23}}{x_{23}} - \frac{y_{12}}{x_{12}}
  \right)
  }{x_{31}}
  \right)
 -
 \left(
 \dfrac{
 \left(
  \frac{y_{12}}{x_{12}} - \frac{y_{01}}{x_{01}}
  \right)
  }{x_{20}}
  \right)
}{
x_{30}
} 
\nonumber
\end{align}
